<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Six reasons to love Camel K</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/8OmZBf5hTzo/" /><category term="Event-Driven" /><category term="Knative" /><category term="Kubernetes" /><category term="Microservices" /><category term="Serverless" /><category term="apache camel" /><category term="Camel K" /><category term="cloud integration" /><category term="cloud native" /><category term="Kubernetes Operator" /><author><name>Christina Lin</name></author><id>https://developers.redhat.com/blog/?p=709837</id><updated>2020-05-12T07:00:12Z</updated><published>2020-05-12T07:00:12Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/"&gt;Apache Camel K&lt;/a&gt; is a lightweight cloud-integration platform that runs natively on Kubernetes and, in particular, lets you automate your cloud configurations. Based on the famous Apache Camel, Camel K is designed and optimized for serverless and microservices architectures. In this article, I discuss six ways that Camel K transforms how developers work with &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;Knative&lt;/a&gt; on cloud platforms.&lt;/p&gt; &lt;p&gt;&lt;span id="more-709837"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Cloud automation and Camel K&lt;/h2&gt; &lt;p&gt;As an old-time developer, I have done my best to adapt to the latest and greatest cloud-native &amp;#8220;ecology.&amp;#8221; It&amp;#8217;s not that difficult, but there are enough traps to ensure that the process isn&amp;#8217;t a smooth ride, either. The slow adoption path is understandable for emerging technologies, but with the large-scale adoption of cloud computing, automation is reaching a new level of maturity. We are now able to focus on how to make things work faster, as well as making the technology more accessible to a broader audience.&lt;/p&gt; &lt;p&gt;Here are six reasons that you will love Camel K.&lt;/p&gt; &lt;h3&gt;Reason 1: Real-time coding on Kubernetes&lt;/h3&gt; &lt;p&gt;When you want to publish an application onto Kubernetes, you need to build the app, containerize it for a working image, and push the image to Kubernetes to run. If you need to debug the app or make other changes, you have to go back to square one and build it again. The worst nightmare for developers is the wait. It is time-consuming, and it breaks the flow of your thoughts. Camel K eliminates this painstaking process by letting developers quickly and immediately update their apps on Kubernetes. You simply layer the image build on the cloud and stream the code using the Camel K Operator.&lt;/p&gt; &lt;h3&gt;Reason 2: Natively on the cloud&lt;/h3&gt; &lt;p&gt;Camel K is smart enough to discover any resources required to run your application. For instance, if you expose an HTTP endpoint in the app, Camel K will create a related service and route on the platform. Camel K also configures &lt;code&gt;cron&lt;/code&gt; jobs based on the behavior you&amp;#8217;ve defined in the code. Camel K handles configurations, and can automatically convert properties files into Kubernetes resources. If you change your mind about a resource, simply remove it from your app, and Camel K will delete it.&lt;/p&gt; &lt;h3&gt;Reason 3: No more dependency hell&lt;/h3&gt; &lt;p&gt;From a developer&amp;#8217;s perspective, configuring dependencies is troublesome and annoying. If you frequently work on monolithic projects, it might not be a big deal: You&amp;#8217;ve probably imported everything you could ever need from previous projects. That tactic works as long as we don&amp;#8217;t talk about redundancy, wasted resources, and version conflicts. But if you are writing a 20-line microservice or function, it doesn&amp;#8217;t make sense to write a 50-line dependency definition. This is where Camel K saves the day: It smartly picks up the dependencies required to run the app, and it automatically locates the associated libraries during the build. You also have the option to load any specific libraries you might need.&lt;/p&gt; &lt;h3&gt;Reason 4: Choose your language and runtime&lt;/h3&gt; &lt;p&gt;Camel K supports a range of languages for software development, no matter if it&amp;#8217;s Java, a scripting language like Groovy or JavaScript, or a markup language like XML or YAML. Camel K also does not need a heavyweight framework like Spring Boot to run. Depending on the type of application, you could run a plain Java main class in a microservice, or go with Quarkus for serverless apps and a fast boot-up time. All you need for the Quarkus runtime is a single configuration: &lt;code&gt;-t quarkus.enabled=true&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Reason 5: Small but powerful&lt;/h3&gt; &lt;p&gt;Camel K&amp;#8217;s core is based on Apache Camel, so it contains many well-established patterns that you can reuse in your applications. The Camel core offers pre-built data transformations and more than 300 components for quick connectivity. You do not need to reinvent the wheel. As a result, your code will be cleaner and easier to maintain, while still being highly customizable and giving you the freedom to choose.&lt;/p&gt; &lt;h3&gt;Reason 6: Serverless made easy&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;Knative&lt;/a&gt; is the most popular serverless framework for Kubernetes. It introduces the idea of scaling to zero and scaling up for load and also, with Knative Eventing, serves as the new specification for future cloud events. Camel K can automatically handle the configurations to apply Knative&amp;#8217;s serverless capabilities to any cloud-based application. Knative makes the process of accessing the Knative &lt;em&gt;channel&lt;/em&gt; (similar to a broker) to get cloud events seamless; it can also reduce the cost of operations, development, and scaling.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Camel K might not be the silver bullet that magically solves all your problems, but it&amp;#8217;s definitely worth your time. Try it out for yourself to see how dramatically it could change the way that you code on cloud platforms. To start, check out this video I madeâ€”a &lt;a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=LaBvBonUC6g"&gt;1,000-foot high overview of Camel K&lt;/a&gt;. You could also learn more from my &lt;a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=dDEpdgg3gK4"&gt;Getting started with Camel K basics&lt;/a&gt; series.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#38;linkname=Six%20reasons%20to%20love%20Camel%20K" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F12%2Fsix-reasons-to-love-camel-k%2F&amp;#038;title=Six%20reasons%20to%20love%20Camel%20K" data-a2a-url="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/" data-a2a-title="Six reasons to love Camel K"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/"&gt;Six reasons to love Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/8OmZBf5hTzo" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Apache Camel K is a lightweight cloud-integration platform that runs natively on Kubernetes and, in particular, lets you automate your cloud configurations. Based on the famous Apache Camel, Camel K is designed and optimized for serverless and microservices architectures. In this article, I discuss six ways that Camel K transforms how developers work with Kubernetes, [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/"&gt;Six reasons to love Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">709837</post-id><dc:creator>Christina Lin</dc:creator><dc:date>2020-05-12T07:00:12Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/</feedburner:origLink></entry><entry><title>Top 10 must-know Kubernetes design patterns</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/8z7jVyPwL30/" /><category term="Containers" /><category term="DevOps" /><category term="Kubernetes" /><category term="Operator" /><category term="cloud native" /><category term="Design Patterns" /><category term="openshift" /><category term="orchestration" /><author><name>Bilgin Ibryam</name></author><id>https://developers.redhat.com/blog/?p=706047</id><updated>2020-05-11T07:00:35Z</updated><published>2020-05-11T07:00:35Z</published><content type="html">&lt;p&gt;Here are the must-know top 10 design patterns for beginners synthesized from &lt;a target="_blank" rel="nofollow" href="http://k8spatterns.io/"&gt;the Kubernetes Patterns book&lt;/a&gt;. Getting familiar with these patterns will help you understand foundational Kubernetes concepts, which in turn will help you in discussions and when designing Kubernetes-based applications.&lt;/p&gt; &lt;p&gt;There are many important concepts in Kubernetes, but these are the most important ones to start with:&lt;/p&gt; &lt;div id="attachment_706057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/top_10_kubernetes_patterns.png"&gt;&lt;img aria-describedby="caption-attachment-706057" class="wp-image-706057 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/top_10_kubernetes_patterns-1024x526.png" alt="Top 10 Kubernetes Design Patterns laid out in a graphic" width="640" height="329" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/top_10_kubernetes_patterns-1024x526.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/top_10_kubernetes_patterns-300x154.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/top_10_kubernetes_patterns-768x394.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706057" class="wp-caption-text"&gt;&lt;em&gt;Source: Kubernetes Patterns&lt;/em&gt;&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To help you understand, the patterns are organized into a few categories below, inspired by the Gang of Four&amp;#8217;s design patterns.&lt;/p&gt; &lt;h1&gt;Foundational patterns&lt;/h1&gt; &lt;p&gt;These patterns represent the principles and best practices that containerized applications must comply with in order to become good cloud-native citizens. Regardless of the application&amp;#8217;s nature, you should aim to follow these guidelines. Adhering to these principles will help ensure that your applications are suitable for automation on Kubernetes.&lt;/p&gt; &lt;h3&gt;Health Probe pattern&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Health Probe&lt;/em&gt; dictates that every container should implement specific APIs to help the platform observe and manage the application in the healthiest way possible. To be fully automatable, a cloud-native application must be highly observable by allowing its state to be inferred so that Kubernetes can detect whether the application is up and ready to serve requests. These observations influence the life-cycle management of Pods and the way traffic is routed to the application.&lt;/p&gt; &lt;h3&gt;Predictable Demands pattern&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Predictable Demands&lt;/em&gt; explains why every container should declare its resource profile and stay confined to the indicated resource requirements. The foundation of successful application deployment, management, and coexistence on a shared cloud environment is dependent on identifying and declaring the application&amp;#8217;s resource requirements and runtime dependencies. This pattern describes how you should declare application requirements, whether they are hard runtime dependencies or resource requirements. Declaring your requirements is essential for Kubernetes to find the right place for your application within the cluster.&lt;/p&gt; &lt;h3&gt;Automated Placement patterns&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Automated Placement&lt;/em&gt;Â explains how to influence workload distribution in a multi-node cluster. Placement is the core function of the Kubernetes scheduler for assigning new Pods to nodes satisfying container resource requests and honoring scheduling policies. This pattern describes the principles of Kubernetesâ€™ scheduling algorithm and the way to influence the placement decisions from the outside.&lt;/p&gt; &lt;h1&gt;Structural patterns&lt;/h1&gt; &lt;p&gt;Having good cloud-native containers is the first step, but not enough. Reusing containers and combining them into Pods to achieve the desired outcome is the next step. The patterns in this category are focused on structuring and organizing containers in a Pod to satisfy different use cases. The forces that affect containers in Pods result in these patterns.&lt;/p&gt; &lt;h3&gt;Init Container pattern&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Init Container&lt;/em&gt; introduces a separate life cycle for initialization-related tasks and the main application containers. Init Containers enable separation of concerns by providing a separate life cycle for initialization-related tasks distinct from the main application containers. This pattern introduces a fundamental Kubernetes concept that is used in many other patterns when initialization logic is required.&lt;/p&gt; &lt;h3&gt;Sidecar patterns&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Sidecar&lt;/em&gt;Â describes how to extend and enhance the functionality of a pre-existing container without changing it. This pattern is one of the fundamental container patterns that allows single-purpose containers to cooperate closely together.&lt;/p&gt; &lt;h1&gt;Behavioral patterns&lt;/h1&gt; &lt;p&gt;These patterns describe the life-cycle guarantees of the Pods ensured by the managing platform. Depending on the type of workload, a Pod might run until completion as a batch job or be scheduled to run periodically. It might run as a daemon service or singleton. Picking the right life-cycle management primitive will help you run a Pod with the desired guarantees.&lt;/p&gt; &lt;h3&gt;Batch Job patterns&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Batch Job&lt;/em&gt; describes how to run an isolated, atomic unit of work until completion. This pattern is suited for managing isolated atomic units of work in a distributed environment.&lt;/p&gt; &lt;h3&gt;Stateful Service patterns&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Stateful Service&lt;/em&gt;Â describes how to create and manage distributed stateful applications with Kubernetes. Such applications require features such as persistent identity, networking, storage, and ordinality. The StatefulSet primitive provides these building blocks with strong guarantees ideal for the management of stateful applications.&lt;/p&gt; &lt;h3&gt;Service Discovery pattern&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Service Discovery&lt;/em&gt; explains how clients can access and discover the instances that are providing application services. For this purpose, Kubernetes provides multiple mechanisms, depending on whether the service consumers and producers are located on or off the cluster.&lt;/p&gt; &lt;h1&gt;Higher-level patterns&lt;/h1&gt; &lt;p&gt;The patterns in this category are more complex and represent higher-level application management patterns. Some of the patterns here (such as Controller) are timeless, and Kubernetes itself is built on top of them.&lt;/p&gt; &lt;h3&gt;Controller pattern&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Controller&lt;/em&gt; is a pattern that actively monitors and maintains a set of Kubernetes resources in a desired state. The heart of Kubernetes itself consists of a fleet of controllers that regularly watch and reconcile the current state of applications with the declared target state. This pattern describes how to leverage this core concept for extending the platform for our own applications.&lt;/p&gt; &lt;h3&gt;Operator pattern&lt;/h3&gt; &lt;p&gt;An &lt;em&gt;Operator&lt;/em&gt; is a Controller that uses a CustomResourceDefinitions to encapsulate operational knowledge for a specific application in an algorithmic and automated form. The Operator pattern allows us to extend the Controller pattern for more flexibility and greater expressiveness. There are an increasing number of &lt;a target="_blank" rel="nofollow" href="http://operatorhub.io/"&gt;Operators&lt;/a&gt; for Kubernetes, and this pattern is turning into the major form of operating complex distributed systems.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today, Kubernetes is the most popular container orchestration platform. It is jointly developed and supported by all major software companies and offered as a service by all of the major cloud providers. Kubernetes supports both Linux and Windows systems, plus all major programming languages. This platform can also orchestrate and automate stateless and stateful applications, batch jobs, periodic tasks, and serverless workloads. The patterns described here are the most commonly used ones from a broader set of patterns that come with Kubernetes as shown below.&lt;/p&gt; &lt;p&gt;&lt;img class="aligncenter size-large wp-image-715927" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/KubernetePatternsLevels-SingleColor-Copy-of-Full-1024x660.png" alt="" width="640" height="413" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/KubernetePatternsLevels-SingleColor-Copy-of-Full-1024x660.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/KubernetePatternsLevels-SingleColor-Copy-of-Full-300x193.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/KubernetePatternsLevels-SingleColor-Copy-of-Full-768x495.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p style="text-align: center;"&gt;&lt;em&gt;Kubernetes Patters organized in different categories&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Kubernetes is the new application portability layer and the common denominator among everybody on the cloud. If you are a software developer or architect, the odds are that Kubernetes will become part of your life in one form or another. Learning about the Kubernetes patterns described here will change the way you think about this platform. I believe that Kubernetes and the concepts originating from it will become as fundamental as object-oriented programming concepts.&lt;/p&gt; &lt;p&gt;The patterns here are an attempt to create the Gang of Four design patterns, but for container orchestration. Reading this article must not be the end, but the beginning of your Kubernetes journey. Happy &lt;code&gt;kubectl&lt;/code&gt;-ing!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#38;linkname=Top%2010%20must-know%20Kubernetes%20design%20patterns" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F11%2Ftop-10-must-know-kubernetes-design-patterns%2F&amp;#038;title=Top%2010%20must-know%20Kubernetes%20design%20patterns" data-a2a-url="https://developers.redhat.com/blog/2020/05/11/top-10-must-know-kubernetes-design-patterns/" data-a2a-title="Top 10 must-know Kubernetes design patterns"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/11/top-10-must-know-kubernetes-design-patterns/"&gt;Top 10 must-know Kubernetes design patterns&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/8z7jVyPwL30" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Here are the must-know top 10 design patterns for beginners synthesized from the Kubernetes Patterns book. Getting familiar with these patterns will help you understand foundational Kubernetes concepts, which in turn will help you in discussions and when designing Kubernetes-based applications. There are many important concepts in Kubernetes, but these are the most important ones [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/11/top-10-must-know-kubernetes-design-patterns/"&gt;Top 10 must-know Kubernetes design patterns&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">706047</post-id><dc:creator>Bilgin Ibryam</dc:creator><dc:date>2020-05-11T07:00:35Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/11/top-10-must-know-kubernetes-design-patterns/</feedburner:origLink></entry><entry><title>Code Ready Containers - Installing an HR employee rewards project using developer container catalog</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/4JW4mBfYv-s/code-ready-containers-installing-hr-employee-rewards-project.html" /><category term="AppDev" scheme="searchisko:content:tags" /><category term="Automate" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="CodeReadyContainers" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Decision Manager" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="workshops" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-code_ready_containers_installing_an_hr_employee_rewards_project_using_developer_container_catalog</id><updated>2020-05-12T08:43:58Z</updated><published>2020-05-11T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div style="text-align: left;"&gt;&lt;/div&gt;&lt;a href="https://1.bp.blogspot.com/-AhmFMDi_plo/Xrpht5NZBdI/AAAAAAAAxHw/DzzcrBKiDYksTJIs7vDvjrC7xrK8qhsggCNcBGAsYHQ/s1600/contiainers-stack.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img alt="Code Ready Containers" border="0" data-original-height="1600" data-original-width="1067" height="320" src="https://1.bp.blogspot.com/-AhmFMDi_plo/Xrpht5NZBdI/AAAAAAAAxHw/DzzcrBKiDYksTJIs7vDvjrC7xrK8qhsggCNcBGAsYHQ/s320/contiainers-stack.jpg" title="" width="213" /&gt;&lt;/a&gt;If you've been following along here lately, you've noticed that I'm exploring Code Ready Containers quite a bit. I've been looking at how to run an OpenShift Container Platform, self-contained on my local machine with no more than 16GB of RAM.&lt;br /&gt;&lt;br /&gt;It's not about just starting up the container platform, it's about doing something real with it. By real I am talking about running a demo, project, or some coding solution I enjoy tinkering with for my day job.&lt;br /&gt;&lt;br /&gt;With that in mind, I've pulled together a project that installs Code Ready Containers for your local machine using 11 GB of RAM. That's the basic setup for running any of the subsequent projects I've shared with you in the past.&lt;br /&gt;&lt;br /&gt;After that I've started sharing how to install various developer tools using the provided developer container catalog images; &lt;a href="https://www.schabell.org/2020/04/code-ready-containers-installing-process-automation-from-developer-catalog.html" target="_blank"&gt;Red Hat Process Automation&lt;/a&gt; and &lt;a href="https://www.schabell.org/2020/04/code-ready-containers-installing-decision-management-developer-container-catalog.html" target="_blank"&gt;Red Hat Decision Manager&lt;/a&gt;. Now it's time to look at installing real projects that allow you to explore the usage of the tooling.&lt;br /&gt;&lt;br /&gt;Let's take a look at installing a human resources employee rewards project using the developer container catalog on Code Ready Containers.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;This tutorial walks through a few simple steps to getting both the latest Code Ready Containers started on your laptop, as well as installing a working project leveraging the provided container catalog images.&lt;br /&gt;&lt;br /&gt;This tutorial is two parts, first installing the latest Code Ready Containers v1.9.0 to provide for an OpenShift Container Platform v4.3.10 on your local machine. Second, you're installing an HR employee rewards project on the provide Red Hat Process Automaton v7.4 authoring image from the developer catalog.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div data-sourcepos="3:1-4:70" dir="auto"&gt;&lt;/div&gt;&lt;h3 data-sourcepos="7:1-9:131" style="text-align: left;"&gt;Installing the container platform&lt;/h3&gt;&lt;div data-sourcepos="9:1-10:83" dir="auto"&gt;You can of course&amp;nbsp;&lt;a href="https://developers.redhat.com/products/codeready-containers/download/" target="_blank"&gt;download the latest Code Ready Containers&lt;/a&gt;, but to ensure that you are able to actively use this with various provided images found in the developer catalog requires fine tuning. To that end, I've provided an installation project that configures all you need in just a single installation script as follows.&lt;br /&gt;&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-eprB2sHCQMs/XoSM1kHpLMI/AAAAAAAAxAI/OPjTjIMdC5Mhgo3P7JHyX8qS2POWukkRgCNcBGAsYHQ/s1600/ocp-login.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="Code Ready Containers" border="0" data-original-height="761" data-original-width="1489" height="163" src="https://1.bp.blogspot.com/-eprB2sHCQMs/XoSM1kHpLMI/AAAAAAAAxAI/OPjTjIMdC5Mhgo3P7JHyX8qS2POWukkRgCNcBGAsYHQ/s320/ocp-login.png" title="" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;First off, you don't even have to pre-download anything other that this project as it's going to check for each component and point you to the download site if you just follow the following steps:&lt;br /&gt;&lt;ol&gt;&lt;li data-sourcepos="29:1-30:0"&gt;&lt;div data-sourcepos="29:4-29:124"&gt;&lt;a href="https://gitlab.com/redhatdemocentral/ocp-install-demo/-/archive/master/ocp-install-demo-master.zip"&gt;Download and unzip.&lt;/a&gt;&lt;/div&gt;&lt;/li&gt;&lt;li data-sourcepos="31:1-32:0"&gt;&lt;div data-sourcepos="31:4-31:116"&gt;Run '&lt;i&gt;init.sh&lt;/i&gt;' or '&lt;i&gt;init.bat&lt;/i&gt;' file, then sit back. (Note: '&lt;i&gt;init.bat&lt;/i&gt;' should be run with Administrative privileges.)&lt;/div&gt;&lt;/li&gt;&lt;li data-sourcepos="33:1-35:0"&gt;&lt;div data-sourcepos="33:4-33:90"&gt;Follow displayed instructions to log in to your brand new Code Ready Containers OpenShift Container Platform on your laptop.&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;Paying close attention to the above init script output ensures a painless experience and you'll be up and running in just a few minutes. If you're wondering what's happening, here are the steps inside the installation if you are running it for the first time on your machine:&lt;br /&gt;&lt;ol style="text-align: left;"&gt;&lt;li&gt;&amp;nbsp;Your system is checked for the availability of the OpenShift client known as '&lt;i&gt;oc client&lt;/i&gt;', if not available or not the latest version, you'll be pointed to where you download. After downloading and installing '&lt;i&gt;oc client&lt;/i&gt;', restart the installation script.&lt;/li&gt;&lt;li&gt;Your system is checked for the availability of Code Ready Containers version 1.9.0, if not you'll be pointed to the download site. Part of this download is a&amp;nbsp;&lt;i&gt;pull-secret&lt;/i&gt;&amp;nbsp;file, get that too and add it's path to the&amp;nbsp;&lt;i&gt;SECRET_PATH&amp;nbsp;&lt;/i&gt;variable in the installation script, it's found at the top. After installing '&lt;i&gt;crc&lt;/i&gt;', restart the installation script.&lt;/li&gt;&lt;li&gt;The next check is for a valid path to your&amp;nbsp;&lt;i&gt;pull-secret&lt;/i&gt;&amp;nbsp;file, it should be set in the&amp;nbsp;&lt;i&gt;SECRET_PATH&lt;/i&gt;&amp;nbsp;variable, if not the installation stops until you correct this.&lt;/li&gt;&lt;li&gt;Next the installation sets up the container platform configuration by allocating 11 GB of memory and 4 CPUs.&lt;/li&gt;&lt;li&gt;At this time the container platform is started, you are invited to grab a coffee while you wait.&lt;/li&gt;&lt;li&gt;As the platform starts, information is gathered to present you with an overview once the installation has completed so you can log in.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;a href="https://1.bp.blogspot.com/-kzus2DnKN0o/XoWeIc0SDkI/AAAAAAAAxA0/_aMcOJlVfDQJzKr2_ysnQmdlZfMAn1xewCNcBGAsYHQ/s1600/Screenshot%2B2020-03-31%2Bat%2B12.54.16.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="Code Ready Containers" border="0" data-original-height="981" data-original-width="1300" height="241" src="https://1.bp.blogspot.com/-kzus2DnKN0o/XoWeIc0SDkI/AAAAAAAAxA0/_aMcOJlVfDQJzKr2_ysnQmdlZfMAn1xewCNcBGAsYHQ/s320/Screenshot%2B2020-03-31%2Bat%2B12.54.16.png" title="" width="320" /&gt;&lt;/a&gt;That's it. Using the URL provided you can log in and open the developer view to watch the second part of this article, installing the process automation tooling container image.&lt;/div&gt;&lt;br /&gt;&lt;h3 data-sourcepos="7:1-9:131" style="text-align: left;"&gt;Installing HR employee rewards&lt;/h3&gt;&lt;div&gt;Now it's time to install the decision management tooling, easily done by following the steps below:&lt;/div&gt;&lt;/div&gt;&lt;div data-sourcepos="42:1-42:140" dir="auto"&gt;&lt;/div&gt;&lt;div data-sourcepos="42:1-42:140" dir="auto"&gt;&lt;ol&gt;&lt;li data-sourcepos="18:1-19:0"&gt;&lt;div data-sourcepos="18:4-18:148"&gt;&lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rewards-demo/-/archive/master/rhcs-rewards-demo-master.zip" target="_blank"&gt;Download and unzip this demo.&lt;/a&gt;&lt;/div&gt;&lt;/li&gt;&lt;li data-sourcepos="20:1-21:0"&gt;&lt;div data-sourcepos="20:4-20:91"&gt;Run '&lt;i&gt;init.sh&lt;/i&gt;' or '&lt;i&gt;init.bat&lt;/i&gt;' file. '&lt;i&gt;init.bat&lt;/i&gt;' must be run with Administrative privileges:&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;Note that this installation does not require any product downloads, because you're installing using the provided Code Ready Containers developer container catalog entry. The exact entry is the Red Hat Process Automation v7.4.&lt;br /&gt;&lt;br /&gt;As you watch the installation script run you can view the developer&amp;nbsp;&lt;i&gt;Topology&lt;/i&gt;&amp;nbsp;view in Code Ready Containers console you logged in to above. You will see two containers starting up, one is the&amp;nbsp;&lt;i&gt;business central authoring&lt;/i&gt;&amp;nbsp;environment and the other is the&amp;nbsp;&lt;i&gt;KIE-server&lt;/i&gt;&amp;nbsp;deployment container.&lt;br /&gt;&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-yLsadQfnUt4/XrkEQ8VRazI/AAAAAAAAxHk/ST93lm75kbklaXz7IsiGTkOeC1b4hoUsQCNcBGAsYHQ/s1600/rhcs-rewards-rhpamcentr-ocp.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="Code Ready Containers" border="0" data-original-height="772" data-original-width="1600" height="154" src="https://1.bp.blogspot.com/-yLsadQfnUt4/XrkEQ8VRazI/AAAAAAAAxHk/ST93lm75kbklaXz7IsiGTkOeC1b4hoUsQCNcBGAsYHQ/s320/rhcs-rewards-rhpamcentr-ocp.png" title="" width="320" /&gt;&lt;/a&gt;Once they are both running with a dark blue circle to indicate this, you can login to the authoring environment. Either look at the authoring container by clicking on it to open the right pane with information, or easier, just use the URL provided at the end of the installation script.&lt;br /&gt;&lt;br /&gt;Now log in to Red Hat Process Automation Manager to start exploring the containerized process automation project featuring an HR employee rewards example (the address will be generated):&lt;/div&gt;&lt;ul data-sourcepos="44:3-45:0" dir="auto"&gt;&lt;li data-sourcepos="44:3-45:0"&gt;Code Ready Container example: https://rhcs-rewards-demo-rhdmcentr-appdev-in-cloud.apps-crc.testing ( u:erics / p:redhatpam1! )&lt;/li&gt;&lt;/ul&gt;&lt;div data-sourcepos="46:1-46:195" dir="auto"&gt;See the project readme file for details on how to run the project.&lt;br /&gt;&lt;br /&gt;Want to build this HR employee rewards project yourself?&amp;nbsp;Learn how step-by-step in this&amp;nbsp;&lt;a href="https://bpmworkshop.gitlab.io/rhpam/index.html#/1" rel=" noreferrer noopener" target="_blank"&gt;online workshop&lt;/a&gt;.&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=d1njakKq38Q:IQ2IH1IqGng:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=d1njakKq38Q:IQ2IH1IqGng:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=d1njakKq38Q:IQ2IH1IqGng:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=d1njakKq38Q:IQ2IH1IqGng:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=d1njakKq38Q:IQ2IH1IqGng:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/d1njakKq38Q" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/4JW4mBfYv-s" height="1" width="1" alt=""/&gt;</content><summary>If you've been following along here lately, you've noticed that I'm exploring Code Ready Containers quite a bit. I've been looking at how to run an OpenShift Container Platform, self-contained on my local machine with no more than 16GB of RAM. It's not about just starting up the container platform, it's about doing something real with it. By real I am talking about running a demo, project, or some...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-05-11T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/d1njakKq38Q/code-ready-containers-installing-hr-employee-rewards-project.html</feedburner:origLink></entry><entry><title>Change data capture with Debezium: A simple how-to, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w4gz00yp2kw/" /><category term="Event-Driven" /><category term="Java" /><category term="Kubernetes" /><category term="Microservices" /><category term="Spring Boot" /><category term="Apache Kafka" /><category term="Data Integration" /><category term="debezium" /><category term="kafka connect" /><category term="mysql" /><category term="openshift" /><author><name>Eric Deandrea</name></author><id>https://developers.redhat.com/blog/?p=707777</id><updated>2020-05-08T07:00:16Z</updated><published>2020-05-08T07:00:16Z</published><content type="html">&lt;p&gt;One question always comes up as organizations moving towards being cloud-native, twelve-factor, and stateless: How do you get an organizationâ€™s data to these new applications? There are many different patterns out there, but one pattern we will look at today is change data capture. This post is a simple how-to on how to build out a change data capture solution using &lt;a target="_blank" rel="nofollow" href="https://debezium.io"&gt;Debezium&lt;/a&gt; within an &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/"&gt;OpenShift&lt;/a&gt; environment. Future posts will also add to this and add additional capabilities.&lt;/p&gt; &lt;h2&gt;What is change data capture?&lt;/h2&gt; &lt;p&gt;Another Red Hatter, &lt;a href="https://developers.redhat.com/blog/author/snandaku/"&gt;Sadhana Nandakumar&lt;/a&gt;, sums it up well in &lt;a href="https://developers.redhat.com/blog/2019/09/03/cdc-pipeline-with-red-hat-amq-streams-and-red-hat-fuse/"&gt;one of her posts&lt;/a&gt; around change data capture:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&amp;#8220;Change data capture (CDC) is a pattern that enables database changes to be monitored and propagated to downstream systems. It is an effective way of enabling reliable microservices integration and solving typical challenges, such as gradually extracting microservices from existing monoliths.&amp;#8221;&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;This pattern lets data become distributed amongst teams, where each team can self-manage their own data while still keeping up-to-date with the original source of data. There are also other patterns, such as &lt;a target="_blank" rel="nofollow" href="https://martinfowler.com/bliki/CQRS.html"&gt;Command Query Responsibility Segregation&lt;/a&gt; (CQRS), which build on this idea.&lt;/p&gt; &lt;h2&gt;What is Debezium?&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io"&gt;Debezium&lt;/a&gt; is an open source technology, supported by Red Hat as part of &lt;a href="https://developers.redhat.com/integration/"&gt;Red Hat Integration&lt;/a&gt;, which allows database row-level changes to be captured as events and published to &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/integration/what-is-apache-kafka"&gt;Apache Kafka&lt;/a&gt; topics. Debezium connectors are based on the popular &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation.html#connect"&gt;Apache Kafka Connect API&lt;/a&gt; and can be deployed within &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ Streams&lt;/a&gt; Kafka clusters.&lt;/p&gt; &lt;h2&gt;Application overview&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/edeandrea/summit-lab-spring-music/tree/pipeline"&gt;The application we will use&lt;/a&gt; as our &amp;#8220;monolith&amp;#8221; is a Spring Boot application that uses a MySQL database as its back end. The application itself has adopted the &lt;a target="_blank" rel="nofollow" href="https://martinfowler.com/eaaDev/EventSourcing.html"&gt;Event Sourcing&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/"&gt;Outbox&lt;/a&gt; patterns. This means that the application maintains a separate table within the database consisting of domain events. It is this table that we need to monitor for changes to publish into our Kafka topics. In this example, there is a table called &lt;code&gt;outbox_events&lt;/code&gt; that looks like this:&lt;/p&gt; &lt;pre&gt;+-----------------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-----------------+--------------+------+-----+---------+----------------+ | event_id | bigint(20) | NO | PRI | NULL | auto_increment | | aggregate_id | varchar(255) | NO | | NULL | | | aggregate_type | varchar(255) | NO | | NULL | | | event_timestamp | datetime(6) | NO | | NULL | | | event_type | varchar(255) | NO | | NULL | | | payload | json | YES | | NULL | | +-----------------+--------------+------+-----+---------+----------------+ &lt;/pre&gt; &lt;h2&gt;Setting up the database&lt;/h2&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html/debezium_user_guide/index"&gt;Debezium documentation&lt;/a&gt; has a section on &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html/debezium_user_guide/debezium-connector-for-mysql"&gt;how to set up the Debezium connector to work with a MySQL database&lt;/a&gt;. We need to follow that documentation but in a container-native way since we will run everything on &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/"&gt;Red Hat OpenShift&lt;/a&gt;. There are many different ways to accomplish this task, but I will describe the way I decided to do it.&lt;/p&gt; &lt;h3&gt;Create an OpenShift project&lt;/h3&gt; &lt;p&gt;The first thing we need to do is log into our OpenShift cluster. In my example, I use OpenShift 4.3. The database setup does not require cluster admin privileges, so any normal user will work fine:&lt;/p&gt; &lt;pre&gt;$ oc login &amp;#60;CLUSTER_API_URL&amp;#62;&lt;/pre&gt; &lt;p&gt;Next, letâ€™s create a project to host our work:&lt;/p&gt; &lt;pre&gt;$ oc new-project debezium-demo&lt;/pre&gt; &lt;h3&gt;Create the MySQL configuration&lt;/h3&gt; &lt;p&gt;From &lt;a target="_blank" rel="nofollow" href="//access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html/debezium_user_guide/debezium-connector-for-mysql#setup-the-mysql-server"&gt;the Debezium documentation on setting up MySQL&lt;/a&gt;, the first thing we need to do is enable the binlog, GTIDs, and query log events. This is typically done in the MySQL configuration file, usually located in &lt;code&gt;/etc/my.cnf&lt;/code&gt;. In our case, we will use &lt;a target="_blank" rel="nofollow" href="https://github.com/sclorg/mysql-container/tree/master/8.0"&gt;Red Hatâ€™s MySQL 8.0 container image&lt;/a&gt;. This image is already deployed in most OpenShift installations in the &lt;code&gt;openshift&lt;/code&gt; namespace under the &lt;code&gt;mysql:8.0&lt;/code&gt; tag. The source of this image comes from &lt;code&gt;registry.redhat.io/rhscl/mysql-80-rhel7:latest&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;According to the &lt;a target="_blank" rel="nofollow" href="https://github.com/sclorg/mysql-container/tree/master/8.0#default-mycnf-file"&gt;container image documentation&lt;/a&gt;, the default configuration file is at &lt;code&gt;/etc/my.cnf&lt;/code&gt;, but there is an environment variable, &lt;code&gt;MYSQL_DEFAULTS_FILE&lt;/code&gt;, that can be used to override its location. MySQL configuration also lets one configuration file include other configuration files, so we will create a new configuration file that first includes the default configuration and then overrides some of that configuration to enable the required Debezium configuration.&lt;/p&gt; &lt;p&gt;Weâ€™ll do this by first creating a configuration file containing our configuration. Weâ€™ll call this file &lt;code&gt;my-debezium.cnf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;!include /etc/my.cnf [mysqld] server-id = 223344 server_id = 223344 log_bin = ON binlog_format = ROW binlog_row_image = full binlog_rows_query_log_events = ON expire_logs_days = 10 gtid_mode = ON enforce_gtid_consistency = ON &lt;/pre&gt; &lt;p&gt;Now that our MySQL configuration file is created, let&amp;#8217;s create it as a &lt;code&gt;ConfigMap&lt;/code&gt; within our OpenShift project:&lt;/p&gt; &lt;pre&gt;$ oc create configmap db-config --from-file=my-debezium.cnf&lt;/pre&gt; &lt;h3&gt;Create a MySQL user&lt;/h3&gt; &lt;p&gt;The next part of the Debezium MySQL configuration is to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html-single/debezium_user_guide/index#create-a-mysql-user-for-cdc_cdc"&gt;create a MySQL user for the connector&lt;/a&gt;. We will follow the same pattern that we did for the configuration by creating a file containing the needed SQL. This &lt;code&gt;initdb.sql&lt;/code&gt; file will create a user with the ID &lt;code&gt;debezium&lt;/code&gt; and password &lt;code&gt;debezium&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;CREATE USER IF NOT EXISTS 'debezium'@'%' IDENTIFIED WITH mysql_native_password BY 'debezium'; GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'debezium'@'%'; FLUSH PRIVILEGES; &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; In a real production environment, we want to choose usernames and passwords more carefully, as well as only allowing the &lt;code&gt;debezium&lt;/code&gt; user access to the tables it will monitor.&lt;/p&gt; &lt;p&gt;Now create a &lt;code&gt;ConfigMap&lt;/code&gt; within our OpenShift project:&lt;/p&gt; &lt;pre&gt;$ oc create configmap db-init --from-file=initdb.sql&lt;/pre&gt; &lt;p&gt;The last piece of the configuration is to create an OpenShift &lt;code&gt;Secret&lt;/code&gt;Â to hold onto our database credentials. This &lt;code&gt;Secret&lt;/code&gt; will be used by our database as well as the application that connects to the database. For simplicity, we will use &lt;code&gt;music&lt;/code&gt; as our database name, username, password, and admin password:&lt;/p&gt; &lt;pre&gt;$ oc create secret generic db-creds --from-literal=database-name=music --from-literal=database-password=music --from-literal=database-user=music --from-literal=database-admin-password=music &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note (again):&lt;/strong&gt; In a real production environment, we want to choose usernames and passwords more carefully.&lt;/p&gt; &lt;h3&gt;Deploy MySQL&lt;/h3&gt; &lt;p&gt;The last part is to create the database and point it to our two configurations. OpenShift allows us to take the &lt;code&gt;ConfigMap&lt;/code&gt;s we created and mount them as files within the container filesystem. We can then use environment variables to change the behavior of the MySQL container image. Letâ€™s create a descriptor YAML file, &lt;code&gt;mysql.yml&lt;/code&gt;, for our database &lt;code&gt;DeploymentConfig&lt;/code&gt; and &lt;code&gt;Service&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;kind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: name: spring-music-db labels: application: spring-music app: spring-music app.kubernetes.io/part-of: spring-music app.openshift.io/runtime: mysql-database spec: replicas: 1 strategy: type: Recreate recreateParams: post: failurePolicy: Abort execNewPod: command: - /bin/sh - '-c' - sleep 10 &amp;#38;&amp;#38; MYSQL_PWD="$MYSQL_ROOT_PASSWORD" $MYSQL_PREFIX/bin/mysql -h $SPRING_MUSIC_DB_SERVICE_HOST -u root &amp;#60; /config/initdb.d/initdb.sql containerName: spring-music-db volumes: - db-init selector: name: spring-music-db template: metadata: name: spring-music-db labels: name: spring-music-db spec: volumes: - name: db-data emptyDir: {} - name: db-init configMap: name: db-init - name: db-config configMap: name: db-config containers: - env: - name: MYSQL_DEFAULTS_FILE value: /config/configdb.d/my-debezium.cnf - name: MYSQL_USER valueFrom: secretKeyRef: name: db-creds key: database-user - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-creds key: database-password - name: MYSQL_DATABASE valueFrom: secretKeyRef: name: db-creds key: database-name - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: db-creds key: database-admin-password name: spring-music-db image: ' ' imagePullPolicy: IfNotPresent volumeMounts: - name: db-data mountPath: /var/lib/mysql/data - name: db-init mountPath: /config/initdb.d - name: db-config mountPath: /config/configdb.d ports: - containerPort: 3306 protocol: TCP livenessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 3306 timeoutSeconds: 1 readinessProbe: exec: command: - /bin/sh - -i - -c - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D $MYSQL_DATABASE -e 'SELECT 1' failureThreshold: 3 initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: memory: 512Mi securityContext: privileged: false triggers: - type: ConfigChange - type: ImageChange imageChangeParams: automatic: true containerNames: - spring-music-db from: kind: ImageStreamTag name: mysql:8.0 namespace: openshift --- kind: Service apiVersion: v1 metadata: name: spring-music-db labels: application: spring-music app: spring-music annotations: template.openshift.io/expose-uri: mysql://{.spec.clusterIP}:{.spec.ports[?(.name=="mysql")].port} spec: ports: - name: mysql port: 3306 protocol: TCP targetPort: 3306 selector: name: spring-music-db &lt;/pre&gt; &lt;p&gt;From this &lt;code&gt;DeploymentConfig&lt;/code&gt;, you can see that we mount our &lt;code&gt;db-init&lt;/code&gt; and &lt;code&gt;db-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt;s as volumes on the container filesystem inside the &lt;code&gt;/config&lt;/code&gt; directory on lines 72-75:&lt;/p&gt; &lt;pre&gt;volumeMounts: - name: db-data mountPath: /var/lib/mysql/data - name: db-init mountPath: /config/initdb.d - name: db-config mountPath: /config/configdb.d&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;/config/configdb.d/my-debezium.cnf&lt;/code&gt; file is also set as the value for the &lt;code&gt;MYSQL_DEFAULTS_FILE&lt;/code&gt; environment variable on lines 44-45:&lt;/p&gt; &lt;pre&gt;- env: - name: MYSQL_DEFAULTS_FILE value: /config/configdb.d/my-debezium.cnf&lt;/pre&gt; &lt;p&gt;The database initialization script from the &lt;code&gt;db-init&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; is executed as a post &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.3/applications/deployments/deployment-strategies.html#deployments-lifecycle-hooks_deployment-strategies"&gt;lifecycle hook&lt;/a&gt; on lines 15-24:&lt;/p&gt; &lt;pre&gt;post: failurePolicy: Abort execNewPod: command: - /bin/sh - '-c' - sleep 10 &amp;#38;&amp;#38; MYSQL_PWD="$MYSQL_ROOT_PASSWORD" $MYSQL_PREFIX/bin/mysql -h $SPRING_MUSIC_DB_SERVICE_HOST -u root &amp;#60; /config/initdb.d/initdb.sql containerName: spring-music-db volumes: - db-init&lt;/pre&gt; &lt;p&gt;Our MySQL instance here is ephemeral, so whenever a new container instance is created the script will execute in a sidecar container within the pod.&lt;/p&gt; &lt;p&gt;Now create the resources and wait for the database pod to start:&lt;/p&gt; &lt;pre&gt;$ oc create -f mysql.yml&lt;/pre&gt; &lt;h2&gt;Starting the application&lt;/h2&gt; &lt;p&gt;Now that our database is up and running we can start the application. Let&amp;#8217;s go to the OpenShift web console and then to the &lt;strong&gt;Developer&lt;/strong&gt; perspective&amp;#8217;s &lt;strong&gt;Topology&lt;/strong&gt; view, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_707847" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707847" class="wp-image-707847 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-1024x380.png" alt="Navigate to Developer Perspective" width="640" height="238" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-1024x380.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-300x111.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-768x285.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view.png 1396w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707847" class="wp-caption-text"&gt;Figure 1: Navigate to Developer Perspective&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then click the &lt;strong&gt;+Add&lt;/strong&gt; button, followed by the &lt;strong&gt;Container Image&lt;/strong&gt; tile, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_707857" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707857" class="wp-image-707857 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-1024x618.png" alt="Add new container image" width="640" height="386" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-1024x618.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-300x181.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-768x464.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image.png 1259w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707857" class="wp-caption-text"&gt;Figure 2: Add new container image&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Fill in the image name with &lt;code&gt;quay.io/edeandrea/spring-music:latest&lt;/code&gt; and then click the search button, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_707877" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name.png"&gt;&lt;img aria-describedby="caption-attachment-707877" class="wp-image-707877" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name.png" alt="Load container image" width="640" height="535" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name.png 973w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name-300x251.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name-768x642.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707877" class="wp-caption-text"&gt;Figure 3: Load container image&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then fill out the rest of the information from Figures 4 and 5 below, making sure to add the correct labels and environment variables by clicking the links at the bottom with the sentence &amp;#8220;Click on the names to access advanced options for Routing, Deployment, Scaling, Resource Limits, and Labels.&amp;#8221;&lt;/p&gt; &lt;p&gt;The fields and values should be filled out as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Application:&lt;/strong&gt; spring-music&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: spring-music&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment Config:&lt;/strong&gt; selected&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create a route to the application:&lt;/strong&gt; checked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Labels&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;app.openshift.io/runtime=spring&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Auto deploy when new image is available&lt;/strong&gt;: checked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto deploy when deployment configuration changes:&lt;/strong&gt; checked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Add from Config Map or Secret&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NAME:Â &lt;/strong&gt;SPRING_DATASOURCE_USERNAME&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VALUE:&lt;/strong&gt; From Secret &lt;strong&gt;db-creds&lt;/strong&gt; fieldÂ &lt;strong&gt;database-user&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add from Config Map or Secret&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NAME:Â &lt;/strong&gt;SPRING_DATASOURCE_PASSWORD&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VALUE:&lt;/strong&gt; from SecretÂ &lt;strong&gt;db-creds&lt;/strong&gt; field &lt;strong&gt;database-password&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add Value&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NAME:Â &lt;/strong&gt;SPRING_DATASOURCE_URL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VALUE:&lt;/strong&gt; jdbc:mysql://spring-music-db/music&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;div id="attachment_707887" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1.png"&gt;&lt;img aria-describedby="caption-attachment-707887" class="wp-image-707887" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1.png" alt="Complete application details" width="640" height="546" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1.png 791w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1-300x256.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1-768x655.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707887" class="wp-caption-text"&gt;Figure 4: Complete application details, Part 1&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_707897" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2.png"&gt;&lt;img aria-describedby="caption-attachment-707897" class="wp-image-707897" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2.png" alt="Complete application details" width="640" height="473" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2.png 994w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2-300x222.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2-768x568.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707897" class="wp-caption-text"&gt;Figure 5: Complete application details, Part 2&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once done, click theÂ &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/p&gt; &lt;p&gt;Back in the &lt;strong&gt;Topology&lt;/strong&gt; view, you should see the application spin up. Once it is surrounded by the blue ring, click the route button on the top-right corner of the application icon, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_707907" style="width: 424px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707907" class="wp-image-707907 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-route.png" alt="Launch application UI" width="414" height="269" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-route.png 414w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-route-300x195.png 300w" sizes="(max-width: 414px) 100vw, 414px" /&gt;&lt;p id="caption-attachment-707907" class="wp-caption-text"&gt;Figure 6: Launch application UI&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This will launch the application. Feel free to play around with it if you&amp;#8217;d like. Try deleting an album.&lt;/p&gt; &lt;h2&gt;Deploy AMQ Streams&lt;/h2&gt; &lt;p&gt;Now that our database and application are up and running letâ€™s deploy our AMQ Streams cluster. First, we need to install the AMQ Streams Operator into the cluster from the OperatorHub. To do this you need cluster admin privileges for your OpenShift cluster. Log in to the web console as a cluster admin, then on the left expand &lt;strong&gt;OperatorHub&lt;/strong&gt;, search for &lt;strong&gt;AMQ Streams&lt;/strong&gt;, and select &lt;strong&gt;Red Hat Integration &amp;#8211; AMQ Streams&lt;/strong&gt;, as shown in Figure 7.&lt;/p&gt; &lt;div id="attachment_707927" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707927" class="wp-image-707927 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-1024x683.png" alt="Find AMQ Streams operator" width="640" height="427" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-1024x683.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-300x200.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-768x512.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install.png 1050w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707927" class="wp-caption-text"&gt;Figure 7: Find AMQ Streams Operator&lt;/p&gt;&lt;/div&gt; &lt;p&gt;On the installation screen, click theÂ &lt;strong&gt;Install&lt;/strong&gt; button, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_707937" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2.png"&gt;&lt;img aria-describedby="caption-attachment-707937" class="wp-image-707937" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2.png" alt="Install AMQ Streams operator" width="640" height="521" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2.png 889w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2-300x244.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2-768x625.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707937" class="wp-caption-text"&gt;Figure 8: Install AMQ Streams Operator&lt;/p&gt;&lt;/div&gt; &lt;p&gt;On theÂ &lt;strong&gt;Create Operator Subscription&lt;/strong&gt; page, leave the defaults and clickÂ &lt;strong&gt;Subscribe&lt;/strong&gt;, as shown in Figure 9. This action will install the Operator for all of the projects in the cluster.&lt;/p&gt; &lt;div id="attachment_707957" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707957" class="wp-image-707957" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3.png" alt="Create operator subscription" width="640" height="533" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3.png 769w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3-300x250.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3-768x639.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707957" class="wp-caption-text"&gt;Figure 9: Create Operator subscription&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You&amp;#8217;ll then be brought to theÂ &lt;strong&gt;Installed Operators&lt;/strong&gt; screen. Sit tight and wait for the Red &lt;strong&gt;Hat Integration &amp;#8211; AMQ Streams&lt;/strong&gt; Operator to show up withÂ &lt;strong&gt;Succeeded&lt;/strong&gt; status, as shown in Figure 10. It shouldn&amp;#8217;t take more than a minute or two.&lt;/p&gt; &lt;div id="attachment_707967" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707967" class="wp-image-707967 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-1024x396.png" alt="Wait for operator to provision" width="640" height="248" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-1024x396.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-300x116.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-768x297.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4.png 1257w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707967" class="wp-caption-text"&gt;Figure 10: Wait for the Operator to provision&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now let&amp;#8217;s create our Kafka cluster. Click on the &lt;strong&gt;Red Hat Integration &amp;#8211; AMQ Streams&lt;/strong&gt; label to get to the main AMQ Streams Operator page. Then under &lt;strong&gt;Provided APIs&lt;/strong&gt;, click theÂ &lt;strong&gt;Create Instance&lt;/strong&gt; label in the &lt;strong&gt;Kafka&lt;/strong&gt; section, as shown in Figure 11.&lt;/p&gt; &lt;div id="attachment_707977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707977" class="wp-image-707977" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1.png" alt="Create Kafka instance" width="640" height="474" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1.png 977w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1-300x222.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1-768x568.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707977" class="wp-caption-text"&gt;Figure 11: Create Kafka instance&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;strong&gt;Create Kafka YAML&lt;/strong&gt; editor will then come up. Remove everything that&amp;#8217;s there, paste in the following, and click the &lt;b&gt;Create&lt;/b&gt; button at the bottom of the screen:&lt;/p&gt; &lt;pre&gt;kind: Kafka apiVersion: kafka.strimzi.io/v1beta1 metadata: name: db-events namespace: debezium-demo labels: app: spring-music-cdc template: spring-music-cdc app.kubernetes.io/part-of: spring-music-cdc spec: kafka: replicas: 3 listeners: plain: {} jvmOptions: gcLoggingEnabled: false config: auto.create.topics.enable: "true" num.partitions: 1 offsets.topic.replication.factor: 3 default.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 storage: type: persistent-claim size: 100Gi deleteClaim: true template: statefulset: metadata: labels: app.kubernetes.io/part-of: spring-music-cdc app: spring-music-cdc template: spring-music-cdc annotations: app.openshift.io/connects-to: db-events-zookeeper zookeeper: replicas: 3 storage: type: persistent-claim size: 100Gi deleteClaim: true template: statefulset: metadata: labels: app.kubernetes.io/part-of: spring-music-cdc app: spring-music-cdc template: spring-music-cdc entityOperator: topicOperator: {} userOperator: {} &lt;/pre&gt; &lt;p&gt;This action will deploy a three-node Kafka cluster along with a three-node Zookeeper cluster. It will also turn down the JVM&amp;#8217;s garbage collection logging so that if we need to look at the logs in any of the Kafka broker pods they wonâ€™t be polluted with tons of garbage collection debug logs. Both the Kafka and Zookeeper brokers are backed by persistent storage, so the data will survive a broker and cluster restart.&lt;/p&gt; &lt;p&gt;Wait a few minutes for OpenShift to spin everything up. You can switch to the OpenShift &lt;strong&gt;Developer&lt;/strong&gt; perspectiveâ€™s &lt;strong&gt;Topology&lt;/strong&gt; view by clicking what is shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_708017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708017" class="wp-image-708017 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-1024x478.png" alt="Switch to developer perspective" width="640" height="299" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-1024x478.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-300x140.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-768x358.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev.png 1372w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708017" class="wp-caption-text"&gt;Figure 12: Switch to developer perspective&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once the &lt;strong&gt;db-events-entity-operator, db-events-kafka, &lt;/strong&gt;and&lt;strong&gt; db-events-zookeeper&lt;/strong&gt; items all show up with a blue ring around them, as shown in Figure 13, you are done.&lt;/p&gt; &lt;div id="attachment_708027" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708027" class="wp-image-708027 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-1024x565.png" alt="Wait for Kafka deployment" width="640" height="353" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-1024x565.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-768x423.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished.png 1090w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708027" class="wp-caption-text"&gt;Figure 13: Wait for Kafka deployment&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Deploy Kafka Connect&lt;/h2&gt; &lt;p&gt;Debezium runs inside a Kafka Connect cluster, so that means we need a container image with both Kafka Connect and the Debezium libraries together. The easiest way to do this is to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_openshift/index#creating-new-image-from-base-str"&gt;create your own container image from the Kafka Connect base image&lt;/a&gt;. What follows are the steps needed to do this. I also already created an image you can use, so feel free to skip this sub-section if you would like and use the image at &lt;code&gt;quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final&lt;/code&gt; instead.&lt;/p&gt; &lt;h3&gt;Building your own Kafka Connect image&lt;/h3&gt; &lt;p&gt;To build your own Kafka Connect image:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a directory on your local computer (i.e., &lt;code&gt;debezium-connect-image&lt;/code&gt;) and then &lt;code&gt;cd&lt;/code&gt; into that directory.&lt;/li&gt; &lt;li&gt;Create a directory inside called &lt;code&gt;plugins&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Download the Debezium MySQL connector from the &lt;a href="https://debezium.io/releases/"&gt;Debezium Releases page&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This post was written using the &lt;code&gt;1.1.0.Final&lt;/code&gt; version of the MySQL connector, but whatever the latest version listed should do fine.&lt;/p&gt; &lt;ol start="4"&gt; &lt;li&gt;Unpackage the downloaded file into the &lt;code&gt;plugins&lt;/code&gt; directory.&lt;/li&gt; &lt;li&gt;Create a &lt;code&gt;Dockerfile&lt;/code&gt; at the root (i.e., &lt;code&gt;debezium-connect-image&lt;/code&gt;) directory with the following contents (you&amp;#8217;ll need an account on &lt;code&gt;registry.redhat.io&lt;/code&gt; and to log into the registry on your machine in order to pull the AMQ Streams image): &lt;pre&gt;FROM registry.redhat.io/amq7/amq-streams-kafka-24-rhel7:1.4.0 USER root:root COPY ./plugins/ /opt/kafka/plugins USER jboss:jboss&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Your directory tree should now look like what&amp;#8217;s shown in Figure 14. &lt;p&gt;&lt;div id="attachment_708127" style="width: 591px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708127" class="wp-image-708127 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/debezium-filesystem-layout.png" alt="Contents of Kafka Connect image" width="581" height="383" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/debezium-filesystem-layout.png 581w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/debezium-filesystem-layout-300x198.png 300w" sizes="(max-width: 581px) 100vw, 581px" /&gt;&lt;p id="caption-attachment-708127" class="wp-caption-text"&gt;Figure 14: Contents of Kafka Connect image&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Build or tag the image using your favorite tool (i.e., Docker/Buildah/etc.) and push it to your registry of choice.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Create Kafka Connect credentials&lt;/h3&gt; &lt;p&gt;Before we create the KafkaConnect cluster there is one small thing we need to take care of. The Debezium connector requires a connection to the database. Rather than hard-coding the credentials into the configuration, letâ€™s instead create an OpenShift &lt;code&gt;Secret&lt;/code&gt; that contains credentials that can then be mounted into the &lt;code&gt;KafkaConnect&lt;/code&gt; pods.&lt;/p&gt; &lt;p&gt;On your local filesystem, create a file called &lt;code&gt;connector.properties&lt;/code&gt;. The contents of this file should be:&lt;/p&gt; &lt;pre&gt;dbUsername=debezium dbPassword=debezium &lt;/pre&gt; &lt;p&gt;Now letâ€™s create the OpenShift &lt;code&gt;Secret&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ oc create secret generic db-connector-creds --from-file=connector.properties&lt;/pre&gt; &lt;h3&gt;Deploy the Kafka Connect image&lt;/h3&gt; &lt;p&gt;Back in the OpenShift console go back to the &lt;strong&gt;Administrator&lt;/strong&gt; perspective and go into &lt;b&gt;Installed Operators&lt;/b&gt;, then click on the &lt;b&gt;Red Hat Integration &amp;#8211; AMQ Streams operator&lt;/b&gt;, as shown in Figure 15.&lt;/p&gt; &lt;div id="attachment_708147" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708147" class="wp-image-708147 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png" alt="Installed operators" width="640" height="236" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-300x111.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-768x284.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams.png 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708147" class="wp-caption-text"&gt;Figure 15: Installed Operators&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then underÂ &lt;strong&gt;Provided APIs&lt;/strong&gt;, click theÂ &lt;strong&gt;Create Instance&lt;/strong&gt; label in theÂ &lt;strong&gt;Kafka Connect&lt;/strong&gt; section, as shown in Figure 16.&lt;/p&gt; &lt;div id="attachment_708157" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708157" class="wp-image-708157 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-1024x425.png" alt="Create Kafka Connect instance" width="640" height="266" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-1024x425.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-300x124.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-768x319.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1.png 1256w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708157" class="wp-caption-text"&gt;Figure 16: Create Kafka Connect instance&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;strong&gt;Create KafkaConnect YAML&lt;/strong&gt; editor will then come up. Remove everything that&amp;#8217;s there, paste in the following, and click the &lt;b&gt;Create&lt;/b&gt; button at the bottom of the screen:&lt;/p&gt; &lt;pre&gt;kind: KafkaConnect apiVersion: kafka.strimzi.io/v1beta1 metadata: name: db-events namespace: debezium-demo labels: app: spring-music-cdc template: spring-music-cdc annotations: strimzi.io/use-connector-resources: "true" spec: replicas: 1 image: "quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final" bootstrapServers: "db-events-kafka-bootstrap:9092" jvmOptions: gcLoggingEnabled: false config: group.id: spring-music-db offset.storage.topic: spring-music-db-offsets config.storage.topic: spring-music-db-configs status.storage.topic: spring-music-db-status config.storage.replication.factor: 1 offset.storage.replication.factor: 1 status.storage.replication.factor: 1 config.providers: file config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider externalConfiguration: volumes: - name: connector-config secret: secretName: db-connector-creds template: deployment: metadata: labels: app: spring-music-cdc app.kubernetes.io/part-of: spring-music-cdc template: spring-music-cdc annotations: app.openshift.io/connects-to: db-events-kafka,spring-music-db &lt;/pre&gt; &lt;p&gt;This action will deploy a one-node KafkaConnect cluster. It will also turn down the JVM&amp;#8217;s garbage collection logging so that if we need to look at the logs in any of the &lt;code&gt;KafkaConnect&lt;/code&gt; pods, they wonâ€™t be polluted with tons of garbage collection debug logs.&lt;/p&gt; &lt;p&gt;As you can see from this configuration, we use the &lt;code&gt;quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final&lt;/code&gt; image on line 13:&lt;/p&gt; &lt;pre&gt;image: "quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final"&lt;/pre&gt; &lt;p&gt;Then we tell the &lt;code&gt;KafkaConnect&lt;/code&gt; cluster to connect to the &lt;code&gt;db-events-kafka-bootstrap:9092&lt;/code&gt; bootstrap server on line 14:&lt;/p&gt; &lt;pre&gt;bootstrapServers: "db-events-kafka-bootstrap:9092"&lt;/pre&gt; &lt;p&gt;Weâ€™ve also added some &lt;code&gt;externalConfiguration&lt;/code&gt;, which tells the &lt;code&gt;KafkaConnect&lt;/code&gt; container to mount the secret named &lt;code&gt;db-connector-creds&lt;/code&gt; into the directory &lt;code&gt;/opt/kafka/external-configuration/connector-config&lt;/code&gt; within the running container (lines 27-31):&lt;/p&gt; &lt;pre&gt;externalConfiguration: volumes: - name: connector-config secret: secretName: db-connector-creds&lt;/pre&gt; &lt;p&gt;If you go back to the OpenShift &lt;strong&gt;Developer&lt;/strong&gt; perspectiveâ€™s &lt;strong&gt;Topology&lt;/strong&gt; view, you should now see the &lt;strong&gt;db-events-connect&lt;/strong&gt; deployment with one replica available, as shown in Figure 17. It might take a few minutes for it to become available.&lt;/p&gt; &lt;div id="attachment_708177" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available.png"&gt;&lt;img aria-describedby="caption-attachment-708177" class="wp-image-708177" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available.png" alt="Wait for Kafka Connect to become available" width="640" height="432" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available.png 1018w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available-300x202.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available-768x518.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-708177" class="wp-caption-text"&gt;Figure 17: Wait for Kafka Connect to become available&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Deploy the Debezium connector&lt;/h2&gt; &lt;p&gt;Now that our Kafka Connect cluster is up and running we can deploy our Debezium connector configuration into it. Back in the OpenShift console, go back to the &lt;strong&gt;Administrator&lt;/strong&gt; perspective, then &lt;b&gt;Installed Operators&lt;/b&gt;, and then click the &lt;b&gt;Red Hat Integration &amp;#8211; AMQ Streams operator&lt;/b&gt;, as shown in Figure 18.&lt;/p&gt; &lt;div id="attachment_708147" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708147" class="wp-image-708147 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png" alt="Installed operators" width="640" height="236" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-300x111.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-768x284.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams.png 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708147" class="wp-caption-text"&gt;Figure 18: Installed Operators&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then underÂ &lt;strong&gt;Provided APIs&lt;/strong&gt;, click theÂ &lt;strong&gt;Create Instance&lt;/strong&gt; label in theÂ &lt;strong&gt;Kafka Connector&lt;/strong&gt; section, as shown in Figure 19.&lt;/p&gt; &lt;div id="attachment_708187" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708187" class="wp-image-708187 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-795x1024.png" alt="Create Kafka Connector instance" width="640" height="824" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-795x1024.png 795w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-233x300.png 233w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-768x990.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1.png 987w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708187" class="wp-caption-text"&gt;Figure 19: Create Kafka Connector instance&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;strong&gt;Create KafkaConnector YAML&lt;/strong&gt; editor will then come up. Remove everything that&amp;#8217;s there, paste in the following, and click the &lt;b&gt;Create&lt;/b&gt; button at the bottom of the screen. This action will deploy the connector configuration into the Kafka Connect cluster and start the connector:&lt;/p&gt; &lt;pre&gt;kind: KafkaConnector apiVersion: kafka.strimzi.io/v1alpha1 metadata: name: db-events namespace: debezium-demo labels: app: spring-music-cdc strimzi.io/cluster: db-events spec: class: io.debezium.connector.mysql.MySqlConnector tasksMax: 1 config: database.hostname: spring-music-db database.port: 3306 database.user: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbUsername}" database.password: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbPassword}" database.dbname: music database.server.name: spring-music database.server.id: 223344 database.whitelist: music database.allowPublicKeyRetrieval: true database.history.kafka.bootstrap.servers: db-events-kafka-bootstrap:9092 database.history.kafka.topic: dbhistory.music table.whitelist: music.outbox_events tombstones.on.delete : false transforms: outbox transforms.outbox.type: io.debezium.transforms.outbox.EventRouter transforms.outbox.route.topic.replacement: "outbox.${routedByValue}.events" transforms.outbox.table.field.event.id: event_id transforms.outbox.table.field.event.key: aggregate_id transforms.outbox.table.field.event.timestamp: event_timestamp transforms.outbox.table.field.event.type: event_type transforms.outbox.table.field.event.payload.id: aggregate_id transforms.outbox.route.by.field: aggregate_type transforms.outbox.table.fields.additional.placement: "event_id:envelope:eventId,event_timestamp:envelope:eventTimestamp,aggregate_id:envelope:aggregateId,aggregate_type:envelope:aggregateType" &lt;/pre&gt; &lt;p&gt;This configuration provides lots of information. Youâ€™ll notice that the database username and password are injected into the configuration via the &lt;code&gt;connector.properties&lt;/code&gt; file stored in our OpenShift &lt;code&gt;Secret&lt;/code&gt; on lines 15-16:&lt;/p&gt; &lt;pre&gt;database.user: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbUsername}" database.password: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbPassword}"&lt;/pre&gt; &lt;p&gt;The configuration also instructs Debezium as to which topic to place events on (line 28):&lt;/p&gt; &lt;pre&gt;transforms.outbox.route.topic.replacement: "outbox.${routedByValue}.events"&lt;/pre&gt; &lt;p&gt;Debezium supports placing all events on a single topic or using a derived routing key to decide the topic. In our case, our application only deals with a single type of domain for its events. For our application, all of the events are stored in the &lt;code&gt;outbox.Album.events&lt;/code&gt; topic.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If our application worked with more than one kind of domain event that might be unrelated to another, it might make sense to place each domainâ€™s events into different topics.&lt;/p&gt; &lt;p&gt;Debezium provides a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation/#connect_transforms"&gt;single message transformation&lt;/a&gt; to provide out-of-the-box support for applications implementing the Outbox pattern. More documentation on the specifics of Debeziumâ€™s Outbox Event Router and itâ€™s configuration can be found in the &lt;a target="_blank" rel="nofollow" href="https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html"&gt;Debezium documentation&lt;/a&gt;. Since the connector has this capability built-in, we just need to tell Debezium how to map between the fields it expects in the payload and the fields in our actual database table (lines 29-35):&lt;/p&gt; &lt;pre&gt;transforms.outbox.table.field.event.id: event_id transforms.outbox.table.field.event.key: aggregate_id transforms.outbox.table.field.event.timestamp: event_timestamp transforms.outbox.table.field.event.type: event_type transforms.outbox.table.field.event.payload.id: aggregate_id transforms.outbox.route.by.field: aggregate_type transforms.outbox.table.fields.additional.placement: "event_id:envelope:eventId,event_timestamp:envelope:eventTimestamp,aggregate_id:envelope:aggregateId,aggregate_type:envelope:aggregateType"&lt;/pre&gt; &lt;p&gt;We could have named the fields in our table exactly as the Debezium &lt;code&gt;EventRouter&lt;/code&gt; transformation was looking for it, but that would then have tightly-coupled our database schema to Debezium. As a best practice, we want our components to be loosely-coupled and updateable via external configuration.&lt;/p&gt; &lt;p&gt;Now, how do we know this all worked? We can go directly to one of the Kafka broker pods and run the &lt;code&gt;kafka-console-consumer&lt;/code&gt; utility to see the data in the topic.&lt;/p&gt; &lt;h3&gt;Look at resulting events&lt;/h3&gt; &lt;p&gt;Go back to the OpenShift web console and the &lt;strong&gt;Topology&lt;/strong&gt; view. Click the &lt;code&gt;db-events-kafka&lt;/code&gt; resource. When the sidebar appears on the right, click any of the three &lt;code&gt;db-events-kafka&lt;/code&gt; pods that show up (i.e., the list in Figure 20). It doesnâ€™t matter which one.&lt;/p&gt; &lt;div id="attachment_708227" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708227" class="wp-image-708227 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-1024x647.png" alt="Select Kafka broker pod" width="640" height="404" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-1024x647.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-300x189.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-768x485.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod.png 1378w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708227" class="wp-caption-text"&gt;Figure 20: Select Kafka broker pod&lt;/p&gt;&lt;/div&gt; &lt;p&gt;From there, click the &lt;b&gt;Terminal&lt;/b&gt; tab to bring you to the terminal. Once at the terminal, run:&lt;/p&gt; &lt;pre&gt;$ bin/kafka-console-consumer.sh --bootstrap-server db-events-kafka-bootstrap:9092 --topic outbox.Album.events --from-beginning &lt;/pre&gt; &lt;p&gt;It will output a bunch of JSON, as shown in Figure 21.&lt;/p&gt; &lt;div id="attachment_708237" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708237" class="wp-image-708237 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-1024x760.png" alt="Run kafka-console-consumer.sh" width="640" height="475" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-1024x760.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-300x223.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-768x570.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer.png 1378w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708237" class="wp-caption-text"&gt;Figure 21: Run kafka-console-consumer.sh&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can now examine the raw output. It should look something like this:&lt;/p&gt; &lt;pre&gt;{"schema":{"type":"struct","fields":[{"type":"string","optional":true,"name":"io.debezium.data.Json","version":1,"field":"payload"},{"type":"string","optional":false,"field":"eventType"},{"type":"int64","optional":false,"field":"eventId"},{"type":"int64","optional":false,"name":"io.debezium.time.MicroTimestamp","version":1,"field":"eventTimestamp"},{"type":"string","optional":false,"field":"aggregateId"},{"type":"string","optional":false,"field":"aggregateType"}],"optional":false},"payload":{"payload":"{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}","eventType":"ALBUM_CREATED","eventId":1,"eventTimestamp":1586264029784000,"aggregateId":"9d0a7606-d933-4026-9f33-efa2bde4b9e4","aggregateType":"Album"}}&lt;/pre&gt; &lt;p&gt;The output might not look too legible, but if you pretty-print it (Google &lt;code&gt;json pretty print&lt;/code&gt; in your browser and find a free utility) youâ€™ll see that the payload format looks like this:&lt;/p&gt; &lt;pre&gt;{ "schema": { "type": "struct", "fields": [ { "type": "string", "optional": true, "name": "io.debezium.data.Json", "version": 1, "field": "payload" }, { "type": "string", "optional": false, "field": "eventType" }, { "type": "int64", "optional": false, "field": "eventId" }, { "type": "int64", "optional": false, "name": "io.debezium.time.MicroTimestamp", "version": 1, "field": "eventTimestamp" }, { "type": "string", "optional": false, "field": "aggregateId" }, { "type": "string", "optional": false, "field": "aggregateType" } ], "optional": false }, "payload": { "payload": "{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}", "eventType": "ALBUM_CREATED", "eventId": 1, "eventTimestamp": 1586264029784000, "aggregateId": "9d0a7606-d933-4026-9f33-efa2bde4b9e4", "aggregateType": "Album" } } &lt;/pre&gt; &lt;p&gt;This payload defines its own structure in the &lt;code&gt;schema&lt;/code&gt; element on lines 2-41:&lt;/p&gt; &lt;pre&gt;"schema": { "type": "struct", "fields": [ { "type": "string", "optional": true, "name": "io.debezium.data.Json", "version": 1, "field": "payload" }, { "type": "string", "optional": false, "field": "eventType" }, { "type": "int64", "optional": false, "field": "eventId" }, { "type": "int64", "optional": false, "name": "io.debezium.time.MicroTimestamp", "version": 1, "field": "eventTimestamp" }, { "type": "string", "optional": false, "field": "aggregateId" }, { "type": "string", "optional": false, "field": "aggregateType" } ], "optional": false }&lt;/pre&gt; &lt;p&gt;We could eliminate this section by standing up our own &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html-single/getting_started_with_service_registry/index"&gt;schema registry&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_openshift/index#service-registry-str"&gt;configuring our cluster to use Avro serialization/deserialization&lt;/a&gt;. The &lt;code&gt;payload&lt;/code&gt; element on lines 42-49 contains metadata about the event, as well as the actual payload of the event:&lt;/p&gt; &lt;pre&gt;"payload": { "payload": "{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}", "eventType": "ALBUM_CREATED", "eventId": 1, "eventTimestamp": 1586264029784000, "aggregateId": "9d0a7606-d933-4026-9f33-efa2bde4b9e4", "aggregateType": "Album" }&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;payload&lt;/code&gt; sub-element within the main payload element (line 43) is itself a JSON string representing the contents of the domain object making up the event:&lt;/p&gt; &lt;pre&gt;"payload": "{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}"&lt;/pre&gt; &lt;p&gt;If you keep this terminal window open and open up a new browser window back to the application itself, you should see new events stream in as you update/delete albums from the applicationâ€™s user interface.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Hopefully, you found this post helpful! If so, please watch for a few other posts in this series once they become available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding Prometheus metrics &amp;#38; Grafana Dashboard monitoring&lt;/li&gt; &lt;li&gt;Securing Kafka and KafkaConnect with OAuth authentication&lt;/li&gt; &lt;li&gt;Adding access control to Kafka and KafkaConnect with OAuth authorization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, if you are like me and want to automate the provisioning of everything, feel free to take a look at an &lt;a target="_blank" rel="nofollow" href="https://github.com/edeandrea/debezium-demo-apb"&gt;Ansible Playbook&lt;/a&gt; that is capable of doing this.&lt;/p&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io/documentation/faq/#what_is_change_data_capture"&gt;What is change data capture?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io"&gt;Debezium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_openshift/index"&gt;Using Red Hat AMQ Streams&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#038;title=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/" data-a2a-title="Change data capture with Debezium: A simple how-to, Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/"&gt;Change data capture with Debezium: A simple how-to, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w4gz00yp2kw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;One question always comes up as organizations moving towards being cloud-native, twelve-factor, and stateless: How do you get an organizationâ€™s data to these new applications? There are many different patterns out there, but one pattern we will look at today is change data capture. This post is a simple how-to on how to build out [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/"&gt;Change data capture with Debezium: A simple how-to, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">707777</post-id><dc:creator>Eric Deandrea</dc:creator><dc:date>2020-05-08T07:00:16Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/</feedburner:origLink></entry><entry><title>Keycloak 10.0.1 released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Za-5OqEHD7g/keycloak-1001-released.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><category term="Keycloak Release" scheme="searchisko:content:tags" /><author><name>Keycloak</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_10_0_1_released</id><updated>2020-05-08T00:00:00Z</updated><published>2020-05-08T00:00:00Z</published><content type="html">&lt;p&gt;To download the release go to &lt;a href="https://www.keycloak.org//downloads.html"&gt;Keycloak downloads&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;All resolved issues&lt;/h2&gt; &lt;p&gt;The full list of resolved issues are available in &lt;a href="https://issues.jboss.org/issues/?jql=project%20%3D%20keycloak%20and%20fixVersion%20%3D%2010.0.1"&gt;JIRA&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Upgrading&lt;/h2&gt; &lt;p&gt;Before you upgrade remember to backup your database and check the &lt;a href="https://www.keycloak.org//docs/latest/upgrading/index.html"&gt;upgrade guide&lt;/a&gt; for anything that may have changed.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Za-5OqEHD7g" height="1" width="1" alt=""/&gt;</content><summary>To download the release go to Keycloak downloads. All resolved issues The full list of resolved issues are available in JIRA Upgrading Before you upgrade remember to backup your database and check the upgrade guide for anything that may have changed.</summary><dc:creator>Keycloak</dc:creator><dc:date>2020-05-08T00:00:00Z</dc:date><feedburner:origLink>https://www.keycloak.org//2020/05/keycloak-1001-released.html</feedburner:origLink></entry><entry><title>Open Data Hub 0.6 brings component updates and Kubeflow architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1y54fcYELvw/" /><category term="Big Data" /><category term="Kubernetes" /><category term="Machine Learning" /><category term="Operator" /><category term="AI/ML" /><category term="Kubeflow" /><category term="Open Data Hub" /><category term="openshift" /><author><name>VÃ¡clav PavlÃ­n</name></author><id>https://developers.redhat.com/blog/?p=717267</id><updated>2020-05-07T07:00:16Z</updated><published>2020-05-07T07:00:16Z</published><content type="html">&lt;p&gt;Open Data Hub (ODH) is a blueprint for building an AI-as-a-service platform on Red Hat&amp;#8217;s &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;-based &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift 4.x&lt;/a&gt;. Version 0.6 of Open Data Hub comes with significant changes to the overall architecture as well as component updates and additions. In this article, we explore these changes.&lt;/p&gt; &lt;h2&gt;From Ansible Operator to Kustomize&lt;/h2&gt; &lt;p&gt;If you follow the &lt;a target="_blank" rel="nofollow" href="https://opendatahub.io"&gt;Open Data Hub&lt;/a&gt; project closely, you might be aware that we have been working on a major design change for a few weeks now. Since we started working closer with the &lt;a target="_blank" rel="nofollow" href="https://www.kubeflow.org/"&gt;Kubeflow community&lt;/a&gt; to get &lt;a target="_blank" rel="nofollow" href="https://www.kubeflow.org/docs/openshift/"&gt;Kubeflow running on OpenShift&lt;/a&gt;, we decided to leverage Kubeflow as the Open Data Hub upstream and adopt its deployment toolsâ€”namely KFdef manifests and &lt;a target="_blank" rel="nofollow" href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt;â€”for deployment manifest customization.&lt;/p&gt; &lt;p&gt;&lt;span id="more-717267"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;We still believe &lt;a target="_blank" rel="nofollow" href="https://sdk.operatorframework.io/docs/ansible/quickstart/"&gt;Ansible Operator&lt;/a&gt; provides a great framework for building Operators, but at the same time, we believe we need to be close to our upstream (Kubeflow), which is why it makes perfect sense to align with the project on the deployment and lifecycle management front. For this purpose, we analyzed all of our components and started to rework them from Ansible roles into a Kustomize-compatible structure.&lt;/p&gt; &lt;p&gt;You can find &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/odh-manifests"&gt;the new manifests in the &lt;code&gt;odh-manifest repository&lt;/code&gt;&lt;/a&gt;. It closely follows the structure of &lt;a target="_blank" rel="nofollow" href="https://github.com/kubeflow/manifests"&gt;the Kubeflow manifests repository&lt;/a&gt; to make sure that the projects are compatible.&lt;/p&gt; &lt;h2&gt;Updated components&lt;/h2&gt; &lt;p&gt;As part of this rewrite, some of the components were updated as well. We decided to depend on OpenShift 4x in the future starting with version 0.6.0. This decision allowed us to fully take advantage of the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/operators/understanding_olm/olm-understanding-olm.html"&gt;Operator Lifecycle Manager&lt;/a&gt; (OLM) and avoid duplicating the maintenance burdens other teams already undergo by providing their projects in the OLM Catalog.&lt;/p&gt; &lt;p&gt;From a technical perspective, this means that we do not hold deployment manifests for all of the components we deploy and manage, but rather we deploy the Operators through the OLM via a subscription and only instruct the Operator by specific custom resource. The components deployed via OLM are &lt;a target="_blank" rel="nofollow" href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://grafana.com/"&gt;Grafana&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s take a look at the other components.&lt;/p&gt; &lt;h3&gt;Spark&lt;/h3&gt; &lt;p&gt;Another component that received an update is the &lt;a target="_blank" rel="nofollow" href="https://github.com/radanalyticsio/spark-operator/"&gt;Radanalytics.io Spark Operator&lt;/a&gt;. We now use &lt;code&gt;SparkCluster&lt;/code&gt; custom resources instead of &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/radanalyticsio/spark-operator#config-map-approach"&gt;ConfigMaps&lt;/a&gt;&lt;/code&gt; and we updated our Spark dependencies to Spark 2.4.5.&lt;/p&gt; &lt;h3&gt;Argo&lt;/h3&gt; &lt;p&gt;We originally wanted to use Argo directly through the Kubeflow project, but Kubeflow ships Argo 2.3, which is too old for Open Data Hub users. Since we had to convert the component to Kustomize anyway we decided to also update it to the latest stable version, so ODH now provides &lt;a target="_blank" rel="nofollow" href="https://blog.argoproj.io/argo-workflows-v2-7-6ace8c210798"&gt;Argo 2.7&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;JupyterHub&lt;/h3&gt; &lt;p&gt;JupyterHub changes are coming in the form of updates to the library &lt;a target="_blank" rel="nofollow" href="https://github.com/vpavlin/jupyterhub-singleuser-profiles/"&gt;JupyterHub Singleuser Profiles&lt;/a&gt;, which we use for customizing Jupyter server deployments and for our integrations between JupyterHub and other services like Spark. These changes are mainly related to refactoring some of the configuration structures to be more Kubernetes-native and improving our integration with the Spark Operator.&lt;/p&gt; &lt;h3&gt;Superset&lt;/h3&gt; &lt;p&gt;The list of updated components above does not include Superset, but you do not need to worryâ€”Superset is still part of Open Data Hub. The deployment simply did not change and was converted into Kustomize.&lt;/p&gt; &lt;h3&gt;AI Library&lt;/h3&gt; &lt;p&gt;The last not mentioned component is the AI Library. You can see it in the repository, but we did not officially include it in the 0.6 release since we were not able to get Seldon in, which is a dependency of this library. We will make sure that both AI Library and Seldon are part of the next release.&lt;/p&gt; &lt;h2&gt;Airflow added to Open Data Hub&lt;/h2&gt; &lt;p&gt;We have been hearing requests for &lt;a target="_blank" rel="nofollow" href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt; in Open Data Hub for a long time. Since we already have a workflow management system (Argo) available in ODH we wanted to make sure that we are not simply duplicating the components. It turns out that many Open Data Hub users are also Airflow users and having Airflow deployed by ODH would add a lot of value for them.&lt;/p&gt; &lt;p&gt;Our team investigated options for adding Airflow among Open Data Hub&amp;#8217;s components and decided to go with the &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/airflow-on-k8s-operator"&gt;Airflow Operator&lt;/a&gt;. We also provide an &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/odh-manifests/tree/master/airflow/example-celery/base"&gt;example Airflow cluster&lt;/a&gt; definition deploying Airflow with Celery that users can easily enable and try out.&lt;/p&gt; &lt;h2&gt;Installing Open Data Hub 0.6&lt;/h2&gt; &lt;p&gt;As with all of the previous versions of Open Data Hub, we care deeply about the user experience during installation. Thatâ€™s why we restructured the Operator Catalog entry (Figure 1) to provide two channelsâ€”beta and legacy (Figure 2).&lt;/p&gt; &lt;div id="attachment_717287" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11.png"&gt;&lt;img aria-describedby="caption-attachment-717287" class="wp-image-717287 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-1024x524.png" alt="Open Data Hub 0.6 OperatorHub entry" width="640" height="328" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-1024x524.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-300x154.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-768x393.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-717287" class="wp-caption-text"&gt;Figure 1: The Open Data Hub 0.6 OperatorHub entry.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_717297" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39.png"&gt;&lt;img aria-describedby="caption-attachment-717297" class="wp-image-717297 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-1024x594.png" alt="Open Data Hub 0.6 channels" width="640" height="371" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-1024x594.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-300x174.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-768x446.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39.png 1518w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-717297" class="wp-caption-text"&gt;Figure 2: The channels in Open Data Hub 0.6.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;The beta channel&lt;/h3&gt; &lt;p&gt;The beta channel provides new versions of Open Data Hub starting with ODH 0.6.0. The custom resource, now based on KFDef, has changed significantly along with the implementation, so donâ€™t be surprised by the new example.&lt;/p&gt; &lt;h3&gt;The legacy channel&lt;/h3&gt; &lt;p&gt;The legacy channel still provides an ODH 0.5.x option. At the moment of publishing, it is 0.5.1, but we will do our best to keep bug fixes flowing into version 0.5 as we know that some users need to stay on that version for the time since 0.6 only works on OCP 4. We will not add new features or components to ODH 0.5, though.&lt;/p&gt; &lt;h3&gt;Kubeflow on OpenShift&lt;/h3&gt; &lt;p&gt;One important feature to mention is that since we use the same tooling as Kubeflow, you can use Open Data Hub Operator 0.6 to deploy Kubeflow on OpenShift. The operator only supports KFDef v1, which is newer than what Kubeflow 0.7 contains, so we prepared an updated custom resource for you in &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/manifests"&gt;our Kubeflow manifests repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It is not possible to deploy and use Kubeflow and Open Data Hub together at the moment but this feature will be available in upcoming releases.&lt;/p&gt; &lt;h2&gt;Community&lt;/h2&gt; &lt;p&gt;This part does not change a bitâ€”we love to get your feedback. We completely understand that ODH 0.6 comes with significant changes, and it is ever more important for us to know if you hit any issues or have any suggestions.&lt;/p&gt; &lt;p&gt;Since we adopted Kubeflow as our upstream and that community lives on GitHub, we are moving there too. We started our planning for future sprints using &lt;a target="_blank" rel="nofollow" href="https://github.com/orgs/opendatahub-io/projects"&gt;GitHub Projects&lt;/a&gt; and we will iteratively move most of the sources and documentation to GitHub as well, so please bear with us in case the project feels a bit chaotic regarding links and pointersâ€”we are working on it.&lt;/p&gt; &lt;p&gt;In any case, do not hesitate to join our &lt;a target="_blank" rel="nofollow" href="https://gitlab.com/opendatahub/opendatahub-community/-/wikis/Open-Data-Hub-Community-Meeting-Agenda"&gt;community meetings&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://lists.opendatahub.io/admin/lists/"&gt;mailing list&lt;/a&gt;, or simply contact us via &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/odh-manifests/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#038;title=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" data-a2a-url="https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/" data-a2a-title="Open Data Hub 0.6 brings component updates and Kubeflow architecture"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/"&gt;Open Data Hub 0.6 brings component updates and Kubeflow architecture&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1y54fcYELvw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Open Data Hub (ODH) is a blueprint for building an AI-as-a-service platform on Red Hat&amp;#8217;s Kubernetes-based OpenShift 4.x. Version 0.6 of Open Data Hub comes with significant changes to the overall architecture as well as component updates and additions. In this article, we explore these changes. From Ansible Operator to Kustomize If you follow the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/"&gt;Open Data Hub 0.6 brings component updates and Kubeflow architecture&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">717267</post-id><dc:creator>VÃ¡clav PavlÃ­n</dc:creator><dc:date>2020-05-07T07:00:16Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/</feedburner:origLink></entry><entry><title>Using Ansible to automate Google Cloud Platform</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/UM8AZASrSmk/" /><category term="CI/CD" /><category term="DevOps" /><category term="Linux" /><category term="Ansible" /><category term="google compute engine" /><category term="rhel 8" /><category term="VM provisioning" /><category term="vpc network" /><author><name>Sreejith Anujan</name></author><id>https://developers.redhat.com/blog/?p=696317</id><updated>2020-05-06T07:00:06Z</updated><published>2020-05-06T07:00:06Z</published><content type="html">&lt;p&gt;In this article, you will learn how to seamlessly automate the provisioning of Google Cloud Platform (GCP) resources using the new &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/"&gt;Red Hat Ansible&lt;/a&gt; modules and your &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/products/tower"&gt;Red Hat Ansible Tower&lt;/a&gt; credentials.&lt;/p&gt; &lt;h2&gt;About the new GCP modules&lt;/h2&gt; &lt;p&gt;Starting with &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/roadmap/ROADMAP_2_6.html"&gt;Ansible 2.6&lt;/a&gt;, Red Hat has &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/partners/programs/CCSP"&gt;partnered with Google&lt;/a&gt; to ship a new set of modules for automating Google Cloud Platform resource management. The partnership has resulted in &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/scenario_guides/guide_gce.html"&gt;more than 100 GCP modules&lt;/a&gt; and a consistent naming scheme of &lt;code&gt;gcp_*&lt;/code&gt;. While we still have access to the original modules, developers are recommended to use the newer modules whenever possible.&lt;/p&gt; &lt;p&gt;&lt;span id="more-696317"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;This article quickly gets you started with using the new modules in both &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/?extIdCarryOver=true&amp;#38;sc_cid=701f2000000RmAOAA0"&gt;Red Hat Ansible&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/products/tower?extIdCarryOver=true&amp;#38;sc_cid=701f2000000RmAOAA0"&gt;Red Hat Ansible Tower&lt;/a&gt;. We&amp;#8217;ll start by connecting a Google Cloud Platform service account to your Red Hat Ansible Tower environment. We&amp;#8217;ll then be able to use Ansible Tower to run a simple Ansible playbook that will provision a new virtual machine (VM) disk, virtual private cloud (VPC) network, and IP address for an automated instance of Red Hat Enterprise Linux 8. You&amp;#8217;ll also see how to use Ansible Tower&amp;#8217;s Dynamic Inventory feature to discover the newly created Red Hat Enterprise Linux 8 instance on Google Cloud Platform.&lt;/p&gt; &lt;h2&gt;Connect your GCP service account to Ansible Tower&lt;/h2&gt; &lt;p&gt;This section assumes you already have a &lt;a target="_blank" rel="nofollow" href="https://developers.google.com/identity/protocols/oauth2/service-account#creatinganaccount"&gt;Google Cloud Platform service account&lt;/a&gt;. If you know your account details, the first thing to do is provide the authentication credentials to Ansible Tower. From the Ansible Tower console, select &lt;strong&gt;Credentials &amp;#8211;&amp;#62; New Credential&lt;/strong&gt;. Then select &lt;strong&gt;Google Compute Engine&lt;/strong&gt; as the &lt;strong&gt;Credential Type&lt;/strong&gt;. Figure 1 shows this sequence.&lt;/p&gt; &lt;div id="attachment_696397" style="width: 406px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696397" class="wp-image-696397 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/select_credential_type.png" alt="A screenshot of the dialog to select the credential type." width="396" height="246" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/select_credential_type.png 396w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/select_credential_type-300x186.png 300w" sizes="(max-width: 396px) 100vw, 396px" /&gt;&lt;p id="caption-attachment-696397" class="wp-caption-text"&gt;Figure 1. Select the credential type.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, you are asked to provide the details of your Google Cloud Platform service account, as well as naming the project in which you plan to create resources. You can find all of this information in your Google Cloud Platform console.&lt;/p&gt; &lt;p&gt;Figure 2 shows the field where you will enter the email address associated with your Google Cloud Platform service account.&lt;/p&gt; &lt;div id="attachment_696407" style="width: 546px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696407" class="wp-image-696407 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_email_address.png" alt="A screenshot of the field to enter an email address." width="536" height="70" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_email_address.png 536w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_email_address-300x39.png 300w" sizes="(max-width: 536px) 100vw, 536px" /&gt;&lt;p id="caption-attachment-696407" class="wp-caption-text"&gt;Figure 2. Enter the email address associated with your Google Cloud Platform service account&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, you&amp;#8217;ll be asked to enter the contents of the Privacy Enhanced Mail (PEM) file associated with your service account email address, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_696387" style="width: 479px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696387" class="wp-image-696387 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/rsa_private_key.png" alt="A screenshot of the dialog to enter the private key." width="469" height="120" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/rsa_private_key.png 469w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/rsa_private_key-300x77.png 300w" sizes="(max-width: 469px) 100vw, 469px" /&gt;&lt;p id="caption-attachment-696387" class="wp-caption-text"&gt;Figure 3. Enter the private key for your account.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Finally, you&amp;#8217;ll enter the Google Cloud Platform project ID, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_696377" style="width: 548px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696377" class="wp-image-696377 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/project_id.png" alt="A screenshot of the dialog to enter the Google Cloud Platform project ID." width="538" height="82" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/project_id.png 538w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/project_id-300x46.png 300w" sizes="(max-width: 538px) 100vw, 538px" /&gt;&lt;p id="caption-attachment-696377" class="wp-caption-text"&gt;Figure 4. Enter the Google Cloud Platform project ID.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If you prefer, you can upload a JSON file with your service account details instead of copying and pasting them in. Figure 5 shows the option to select and upload a JSON file.&lt;/p&gt; &lt;div id="attachment_696417" style="width: 550px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696417" class="wp-image-696417 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_json_file.png" alt="A screenshot of the dialog to select and upload a JSON file." width="540" height="84" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_json_file.png 540w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_json_file-300x47.png 300w" sizes="(max-width: 540px) 100vw, 540px" /&gt;&lt;p id="caption-attachment-696417" class="wp-caption-text"&gt;Figure 5. Select and upload a JSON file.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 6 shows the Google Cloud Platform service account, associated email address, and project ID for the environment we&amp;#8217;ve set up so far.&lt;/p&gt; &lt;div id="attachment_696367" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696367" class="wp-image-696367 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-1024x241.png" alt="A screenshot of the completed GCP configuration." width="640" height="151" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-1024x241.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-300x71.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-768x181.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id.png 1313w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696367" class="wp-caption-text"&gt;Figure 6. Configuration for the Google Cloud Platform service account.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;While it is not shown, I have also configured the Google Compute Engine API (&lt;code&gt;compute.googleapis.com&lt;/code&gt;) to access the Google Cloud Platform project, and set the Identity and Access Management (IAM) role to Owner.&lt;/p&gt; &lt;h2&gt;Create a new playbook for the Red Hat Enterprise Linux 8 instance&lt;/h2&gt; &lt;p&gt;After configuring our environment, the next step is to create an Ansible playbook. We&amp;#8217;ll use the Ansible playbook to create a VM disk, a VPC network, an IPv4 address, and finally our new instance of Red Hat Enterprise Linux 8.&lt;/p&gt; &lt;p&gt;I&amp;#8217;m showing the Ansible playbook in Figure 7 as a screenshot to preserve the indentation. You can also &lt;a target="_blank" rel="nofollow" href="http://people.redhat.com/sanujan/gcp_resources.yml"&gt;download the playbook&lt;/a&gt; and use it as an example.&lt;/p&gt; &lt;div id="attachment_701197" style="width: 592px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-701197" class="wp-image-701197 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/Ansible_GCP_Playbook.png" alt="A screenshot of the Ansible playbook." width="582" height="886" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/Ansible_GCP_Playbook.png 582w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/Ansible_GCP_Playbook-197x300.png 197w" sizes="(max-width: 582px) 100vw, 582px" /&gt;&lt;p id="caption-attachment-701197" class="wp-caption-text"&gt;Figure 7. The Ansible playbook to create a Red Hat Enterprise Linux 8 instance.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Let&amp;#8217;s quickly look at each task in the playbook:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task 1: Create the VM disk&lt;/strong&gt;: We can use a source image to create a VM disk and make the disk bootable. In this case, we&amp;#8217;ll use the certified Red Hat Enterprise Linux 8 image available from Google Cloud Platform. The &lt;code&gt;gcp_compute_disk&lt;/code&gt; module uses the &lt;code&gt;rhel-8-v20190905&lt;/code&gt; image to add a persistent disk for our Red Hat Enterprise Linux 8 instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 2: Create the VPC network&lt;/strong&gt;: Next, the &lt;code&gt;gcp_compute_network&lt;/code&gt; module creates a VPC network. The Red Hat Enterprise Linux instance will have an interface associated with that network.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 3: Create the IPv4 address&lt;/strong&gt;: The &lt;code&gt;gcp_compute_address&lt;/code&gt; module allocates an external IPv4 address to be associated with the Red Hat Enterprise Linux instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 4: Create the Red Hat Enterprise Linux instance&lt;/strong&gt;: The &lt;code&gt;gcp_compute_instance&lt;/code&gt; module uses the resources from the previous tasks to create an instance of Red Hat Enterprise Linux 8.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 5: Check the IPv4 address&lt;/strong&gt;: The debug module shows the IPv4 address associated with the Red Hat Enterprise Linux 8 instance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note that we will execute the playbook within Ansible Tower. We&amp;#8217;ll use the &lt;code&gt;auth_kind&lt;/code&gt; attribute to reference the Google Cloud Platform credentials required for each task. We&amp;#8217;ll then use the &lt;code&gt;gcp_cred_kind&lt;/code&gt; variable to map the tasks to our Google Cloud Platform &lt;code&gt;serviceaccount&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;During execution, Ansible Tower will use the &lt;code&gt;GCP_AUTH_KIND&lt;/code&gt; environment variable to reference and pass the Google Cloud Platform service account&amp;#8217;s email, project, and private-key contents to the playbook.&lt;/p&gt; &lt;h3&gt;Execute the playbook&lt;/h3&gt; &lt;p&gt;Next, we want to create a new job template to execute the playbook in Ansible Tower. Before we can do that, we need to create an inventory and a project referencing the playbook. For this demo, the project is &lt;strong&gt;GCP&lt;/strong&gt; and the inventory is &lt;strong&gt;GCP_Provision_Resources&lt;/strong&gt;, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_696357" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696357" class="wp-image-696357 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-1024x264.png" alt="A screenshot of the configuration for the playbook inventory and project." width="640" height="165" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-1024x264.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-300x77.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-768x198.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template.png 1239w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696357" class="wp-caption-text"&gt;Figure 8. Create an inventory and a project referencing the playbook you want to run.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Run the job template&lt;/h3&gt; &lt;p&gt;Now, we can run the job template. Check the Google Cloud Platform console to see the new resources being created as each individual task is executed. From the job template output shown in Figure 9, you can see that the new resources were created.&lt;/p&gt; &lt;div id="attachment_696327" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696327" class="wp-image-696327 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-1024x567.png" alt="A screenshot of job template output." width="640" height="354" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-1024x567.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-300x166.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-768x425.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template.png 1053w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696327" class="wp-caption-text"&gt;Figure 9. Job template output shows the new resources have been created.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can also use the Google Cloud Platform console to verify the resources. In Figure 10, you can see the VM disk, &lt;code&gt;disk-instance&lt;/code&gt;, was created with a size of 50 GB.&lt;/p&gt; &lt;div id="attachment_696427" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696427" class="wp-image-696427 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-1024x239.png" alt="A screenshot of the VM disk listed in the Google Compute Engine console." width="640" height="149" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-1024x239.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-300x70.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-768x179.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk.png 1177w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696427" class="wp-caption-text"&gt;Figure 10. The VM disk has been created.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 11 shows that the VPC network, &lt;code&gt;network-instance&lt;/code&gt;, was created on subnet 10.240.0.0/16.&lt;/p&gt; &lt;div id="attachment_696457" style="width: 517px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696457" class="wp-image-696457 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_network.png" alt="A screenshot showing the VPC network hosted on Google Cloud Platform." width="507" height="387" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_network.png 507w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_network-300x229.png 300w" sizes="(max-width: 507px) 100vw, 507px" /&gt;&lt;p id="caption-attachment-696457" class="wp-caption-text"&gt;Figure 11. The VPC network has been created.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 12 shows that the external public IPv4 address of 35.192.86.87 is associated with the newly created Red Hat Enterprise Linux 8 instance.&lt;/p&gt; &lt;div id="attachment_696437" style="width: 649px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png"&gt;&lt;img aria-describedby="caption-attachment-696437" class="wp-image-696437" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png" alt="A screenshot of the external public IPv4 address." width="639" height="143" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png 991w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-300x67.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-768x171.png 768w" sizes="(max-width: 639px) 100vw, 639px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-696437" class="wp-caption-text"&gt;Figure 12. The external public IPv4 address is listed.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Finally, in Figure 13 we see that the new Red Hat Enterprise Linux 8 instance was created. The new instance, &lt;code&gt;rhel8&lt;/code&gt;, has an internal IP address of 10.240.0.2 and a public external IP address of 35.192.86.87.&lt;/p&gt; &lt;div id="attachment_696437" style="width: 651px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png"&gt;&lt;img aria-describedby="caption-attachment-696437" class="wp-image-696437" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png" alt="A screenshot showing the new instance on Google Compute Engine." width="641" height="143" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png 991w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-300x67.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-768x171.png 768w" sizes="(max-width: 641px) 100vw, 641px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-696437" class="wp-caption-text"&gt;Figure 13. The new Red Hat Enterprise Linux 8 instance has been created.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Create a dynamic inventory for GCP instances&lt;/h2&gt; &lt;p&gt;Ansible Tower lets you periodically sync with the Google Cloud API to find realtime instance counts and details for resources hosted on Google Cloud Platform.&lt;/p&gt; &lt;p&gt;To create a new inventory, choose &lt;strong&gt;Google Compute Engine&lt;/strong&gt; as the source, then select the Google Cloud Platform credential you created at the beginning of this article. Optionally, you can limit the sync to specific GCP regions, as shown in Figure 14.&lt;/p&gt; &lt;div id="attachment_696337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696337" class="wp-image-696337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-1024x258.png" alt="A screenshot of the new inventory." width="640" height="161" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-1024x258.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-300x76.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-768x194.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory.png 1598w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696337" class="wp-caption-text"&gt;Figure 14. Create a new inventory.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, use the &lt;strong&gt;Inventory Sync&lt;/strong&gt; button to synchronize the newly created instance &lt;code&gt;rhel8&lt;/code&gt; in the &lt;strong&gt;US Central (A)&lt;/strong&gt; region.&lt;/p&gt; &lt;p&gt;Once synced, check the new hosts in the inventory. As you can see in Figure 15, the new instance is listed.&lt;/p&gt; &lt;div id="attachment_696347" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696347" class="wp-image-696347 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-1024x218.png" alt="A screenshot showing the new instance is listed." width="640" height="136" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-1024x218.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-300x64.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-768x164.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync.png 1305w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696347" class="wp-caption-text"&gt;Figure 15. The new instance is listed.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you&amp;#8217;ve seen how to use the new &lt;code&gt;gcp_*&lt;/code&gt; modules to automate the provisioning of resources on Google Cloud Platform. I also quickly showed you how to create a new dynamic inventory to periodically sync with the Google Cloud API and find realtime instance counts and details for resources hosted on Google Cloud Platform. Using the Dynamic Inventory feature is a recommended practice for large, fast-changing cloud environments where systems are frequently deployed, tested, and then removed.&lt;/p&gt; &lt;p&gt;Here is a video demonstration of the activities covered in this article.&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/v6dwOPkA-bA?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;See the following resources to learn more about topics in this article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/index.html"&gt;About Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible-tower/"&gt;About Ansible Tower&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/modules/list_of_cloud_modules.html#google"&gt;The new Google Cloud Platform modules for Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible-tower/latest/html/quickinstall/prepare.html"&gt;Installation instructions for Ansible Tower&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://cloud.google.com/deployment-manager/docs/step-by-step-guide/installation-and-setup"&gt;Installation and setup instructions for Google Cloud&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#038;title=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" data-a2a-url="https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/" data-a2a-title="Using Ansible to automate Google Cloud Platform"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/"&gt;Using Ansible to automate Google Cloud Platform&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/UM8AZASrSmk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In this article, you will learn how to seamlessly automate the provisioning of Google Cloud Platform (GCP) resources using the new Red Hat Ansible modules and your Red Hat Ansible Tower credentials. About the new GCP modules Starting with Ansible 2.6, Red Hat has partnered with Google to ship a new set of modules for [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/"&gt;Using Ansible to automate Google Cloud Platform&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">696317</post-id><dc:creator>Sreejith Anujan</dc:creator><dc:date>2020-05-06T07:00:06Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/</feedburner:origLink></entry><entry><title>Working with big spatial data workflows (or, what would John Snow do?)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Idzz0tQQ5zY/" /><category term="Big Data" /><category term="Java" /><category term="JavaScript" /><category term="Microservices" /><category term="apache camel" /><category term="cluster map" /><category term="low code" /><category term="Red Hat Fuse" /><category term="syndesis" /><author><name>Maria Arias de Reyna Dominguez</name></author><id>https://developers.redhat.com/blog/?p=705317</id><updated>2020-05-05T07:00:03Z</updated><published>2020-05-05T07:00:03Z</published><content type="html">&lt;p&gt;With the rise of social networks and people having more free time due to isolation, it has become popular to see lots of maps and graphs. These are made using big spatial data to explain how COVID-19 is expanding, why it is faster in some countries, and how we can stop it.&lt;/p&gt; &lt;p&gt;Some of these maps and graphs are made by inexperienced amateurs that have access to huge amounts of raw and processed big spatial data. But most of them are not sure how to handle that data. A few unaware amateurs mix different sources without caring about homogenizing the data first. Some others mix old data with new. And finally, most forget to add relevant variables because this is too much data to handle manually.&lt;/p&gt; &lt;p&gt;How would a professional handle all of this?&lt;/p&gt; &lt;h2&gt;The cholera outbreak&lt;/h2&gt; &lt;p&gt;In situations where we have to handle big spatial data, I can&amp;#8217;t help but wonder: What would John Snow do? I&amp;#8217;m not talking about that &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Jon_Snow_(character)"&gt;warrior in the cold north fighting zombies&lt;/a&gt;. I&amp;#8217;m talking about the original &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/John_Snow"&gt;John Snow&lt;/a&gt;, an English doctor from the XIX century that used spatial data to study a cholera outbreak.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s go back to 1854, London, where a &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/1854_Broad_Street_cholera_outbreak"&gt;cholera outbreak&lt;/a&gt; was taking heavy casualties. Most doctors at the time, unaware of germs, thought it was caused by &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Miasma_theory"&gt;miasma&lt;/a&gt;, a kind of bad air that polluted people, making them ill.&lt;/p&gt; &lt;h3&gt;John Snow data analysis&lt;/h3&gt; &lt;p&gt;But John was not convinced by that theory. He had a hypothesis on what the real cause could be, suspecting water-related issues. He collected data on where the people infected lived and where they got their water from and ran some spatial data analysis to prove those ideas. Figure 1 shows one of his original maps.&lt;/p&gt; &lt;div id="attachment_705337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705337" class="wp-image-705337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-1024x954.jpg" alt="Original map by John Snow showing the clusters of cholera cases (indicated by stacked rectangles) in the London epidemic of 1854" width="640" height="596" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-1024x954.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-300x280.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-768x716.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1.jpg 1280w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-705337" class="wp-caption-text"&gt;Figure 1: Original map by John Snow showing the clusters of cholera cases in the London epidemic of 1854.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;With that accurate data, he was able to generate a cluster map showing the spread of the disease. This work helped him prove his theories on cholera&amp;#8217;s water origin. He had only a few sources of data, but they were all homogeneous. Plus, he was able to collect data directly in the field, making sure it was accurate and met his needs.&lt;/p&gt; &lt;p&gt;It&amp;#8217;s important to notice that because he used the right data, he arrived at the right conclusions. He studied the outliers, like those people drinking water from a different source than what should have been the closest to their homes. Thus he was able to conflate the data with the proper sources, curating it. Homogenizing and conflating the sources of data is a relevant step to arrive at the right conclusions.&lt;/p&gt; &lt;p&gt;John Snow had to manually conflate and analyze all of the data and it was a good choice. The amount of data he handled was fit for working with pen and paper. But in our case, when we try to conflate all the sources available worldwide, what we are really facing is big spatial data, which is impossible to handle manually.&lt;/p&gt; &lt;h2&gt;Big spatial data&lt;/h2&gt; &lt;p&gt;Not only do we have the specific related data, but we also have data about different isolation or social distancing norms, health care, personal savings, access to clean water, diet, population density, population age, and previous health care issues. The amount of related data available is huge.&lt;/p&gt; &lt;p&gt;Remember, if your data fits into a hard disk, that&amp;#8217;s hardly big data. We are talking here about the amount of data that calls for unending data storage on server farms. No analyst can update, conflate, and analyze all that data manually. We need tools, good tools, to be able to deliver reliable results.&lt;/p&gt; &lt;p&gt;Consider that different data collectors update their data in almost real-time but at different rates, and each country has its own statistics and its own way to measure each variable. So, you need to transform and homogenize before conflating those sources.&lt;/p&gt; &lt;p&gt;How can we keep up-to-date without going crazy? Before you can finish even half of the workflow shown in Figure 2, there&amp;#8217;s freshly new data waiting for you.&lt;/p&gt; &lt;div id="attachment_705397" style="width: 769px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705397" class="wp-image-705397 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/workflow.png" alt="Workflow for big spatial data: Update â†’ Homogenize â†’ Conflate â†’ Analyze" width="759" height="127" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/workflow.png 759w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/workflow-300x50.png 300w" sizes="(max-width: 759px) 100vw, 759px" /&gt;&lt;p id="caption-attachment-705397" class="wp-caption-text"&gt;Figure 2: We need to run this workflow continuously to always use the newest big spatial data available.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;What would John Snow do? Well, I&amp;#8217;m quite sure he would like all of us to use the proper tools for the work. That&amp;#8217;s why it&amp;#8217;s called &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Location_intelligence"&gt;Location &lt;em&gt;Intelligence&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Middleware to the rescue&lt;/h2&gt; &lt;p&gt;Regarding those four steps, there are three that can be automated: update, homogenize, and conflate. All of those are tedious and repetitive tasks that make developers quickly jump into scripting rough code. And we know what happens when we write quickly supporting code: We tend to make the same mistakes that others already fixed.&lt;/p&gt; &lt;p&gt;Well, here we are lucky. We have several free and open source software libraries and frameworks that can help us through these tasks. These tools can be found in the &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/technologies/jboss-middleware/fuse"&gt;Red Hat Fuse Integration Platform&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Apache Camel&lt;/h3&gt; &lt;p&gt;Our first option should always be using &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt; to help us create complex data workflows. With this framework, we can periodically extract the latest data from different sources, transform, and conflate automatically. We can even use &lt;a href="https://developers.redhat.com/blog/2019/08/27/devnation-live-kubernetes-enterprise-integration-patterns-with-camel-k/"&gt;Camel K&lt;/a&gt; and leave it running on some Kubernetes container while we focus on the non-automatable steps of our work.&lt;/p&gt; &lt;p&gt;Defining workflows in Camel is easy. You can use &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/manual/latest/languages.html"&gt;different common languages&lt;/a&gt; such as &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/manual/latest/java-dsl.html"&gt;Java&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/languages/javascript.html"&gt;Javascript&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/languages/groovy.html"&gt;Groovy,&lt;/a&gt; or a specific &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/manual/latest/dsl.html"&gt;domain-specific language (DSL)&lt;/a&gt;. With &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/index.html"&gt;Camel&amp;#8217;s hundreds of components&lt;/a&gt;, you can feed your workflow with almost any source of data, process the data, and output the processed data in the format your analysis requires.&lt;/p&gt; &lt;h3&gt;Syndesis&lt;/h3&gt; &lt;p&gt;For those data analysts that are less tech-savvy and feel that writing Camel scripts is too complex, we also have &lt;a target="_blank" rel="nofollow" href="https://syndesis.io/"&gt;Syndesis&lt;/a&gt;. With Syndesis you can &lt;a href="https://developers.redhat.com/blog/2020/03/25/low-code-microservices-orchestration-with-syndesis/"&gt;define data workflows&lt;/a&gt; in a more visual way, as you can see in Figure 3.&lt;/p&gt; &lt;div id="attachment_705627" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705627" class="wp-image-705627 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-1024x507.png" alt="Syndesis Integrations Overview" width="640" height="317" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-1024x507.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-300x149.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-768x380.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1.png 1030w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-705627" class="wp-caption-text"&gt;Figure 3: We can define several processes on Syndesis, each running based on a different trigger.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This means you can update that big spatial data without having to write a single line of code. Or, maybe you just want to speed up the workflow creation process to jump directly into the analysis.&lt;/p&gt; &lt;p&gt;We can either create one single workflow or break it down to several workflows, as shown in Figure 4. For example, the first process could be triggered by a timer to download different data sources and send that raw data to a Kafka broker. Then, a second process could listen to that broker, transform and homogenize the data previously downloaded, and store it on some common data storage. Finally, a third process can take several sources of data from that common storage with homogenized data, conflate those sources, and prepare the data for further analysis or exposition.&lt;/p&gt; &lt;div id="attachment_705637" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705637" class="wp-image-705637 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-1024x777.png" alt="Syndesis Integration Creation View for a process to update big spatial data" width="640" height="486" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-1024x777.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-300x228.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-768x583.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2.png 1031w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-705637" class="wp-caption-text"&gt;Figure 4: We can easily add steps to the workflow using that plus button.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Note that each step can filter, transform, and use data from different sources, allowing us to create complex workflows in a simple and visual way. We can run the data through different APIs, XSLT transformations, data mapping, and filters to make sure we end up with data ready for analysis.&lt;/p&gt; &lt;h2&gt;The final touch&lt;/h2&gt; &lt;p&gt;Now that we have our data updated, homogenized, transformed, and conflated, we can start the analysis. As both Camel and Syndesis can provide the output in different formats, we can connect it to any software we need to do this analysis. From databases like &lt;a target="_blank" rel="nofollow" href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; to XML-based data formats like &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Keyhole_Markup_Language"&gt;KML&lt;/a&gt;, we could feed our analysis tools the way we need.&lt;/p&gt; &lt;p&gt;For example, we can use &lt;a target="_blank" rel="nofollow" href="https://qgis.org"&gt;QGIS&lt;/a&gt;, which is an advanced desktop application for data analysis. You can add all those already transformed and conflated big spatial data sources to QGIS to create beautiful graphs and maps as outputs. After that, you can publish your maps with &lt;a target="_blank" rel="nofollow" href="https://openlayers.org/"&gt;OpenLayers&lt;/a&gt; or &lt;a target="_blank" rel="nofollow" href="https://leafletjs.com/"&gt;Leaflet&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Make John Snow proud! And do it using free and open source software.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#038;title=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" data-a2a-url="https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/" data-a2a-title="Working with big spatial data workflows (or, what would John Snow do?)"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/"&gt;Working with big spatial data workflows (or, what would John Snow do?)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Idzz0tQQ5zY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;With the rise of social networks and people having more free time due to isolation, it has become popular to see lots of maps and graphs. These are made using big spatial data to explain how COVID-19 is expanding, why it is faster in some countries, and how we can stop it. Some of these [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/"&gt;Working with big spatial data workflows (or, what would John Snow do?)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">705317</post-id><dc:creator>Maria Arias de Reyna Dominguez</dc:creator><dc:date>2020-05-05T07:00:03Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/</feedburner:origLink></entry><entry><title>Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/BeZggEwPQzQ/" /><category term="DevOps" /><category term="Event-Driven" /><category term="Java" /><category term="Microservices" /><category term="data visualization" /><category term="elastic search" /><category term="Kibana" /><category term="KPI metrics" /><category term="openshift" /><author><name>snandaku</name></author><id>https://developers.redhat.com/blog/?p=706837</id><updated>2020-05-04T07:00:34Z</updated><published>2020-05-04T07:00:34Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt; is a platform for developing containerized microservices and applications that automate business decisions and processes. Combining process- and task-level SLA metrics plus case-related breakdowns can be beneficial for identifying trends and reorganizing the workforce as necessary. So, aÂ critical piece of a business process system is having real-time insights into what is happening, and both monitoring KPI metrics and responding to problem trends is an integral part of operations.&lt;/p&gt; &lt;p&gt;Integration with Elasticsearch improves our search capabilities and provides a unified reporting environment for the business. &lt;a target="_blank" rel="nofollow" href="http://mswiderski.blogspot.com/2017/08/elasticsearch-empowers-jbpm.html"&gt;Maciej Swiderski blogged about&lt;/a&gt; how we can potentially use Elasticsearch to capture KPI metrics and provide for full-text search capabilities. This article extends the idea and walks through how to enable integration with Elasticsearch on a &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; environment, and how to represent the KPIs in a graphical business-friendly dashboard using Kibana.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Preparing the demo environment&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Letâ€™s install the necessary components for this demonstration on Red Hat OpenShift, which enables efficient container orchestration and allows rapid container provisioning, deployment, scaling, and management.&lt;/p&gt; &lt;h3&gt;Setting up Elastic and Kibana&lt;/h3&gt; &lt;p&gt;Let&amp;#8217;s make use of the Elastic cluster Operator to set up Elastic/Kibana on OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc apply -f &lt;a target="_blank" rel="nofollow" href="https://download.elastic.co/downloads/eck/1.0.1/all-in-one.yaml"&gt;https://download.elastic.co/downloads/eck/1.0.1/all-in-one.yaml&lt;/a&gt; $ oc new-project elastic&lt;/pre&gt; &lt;p&gt;Now, we can deploy an Elastic instance:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60;EOF | oc apply -n elastic -f - # This sample sets up an Elasticsearch cluster with an OpenShift route apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: Â name: elasticsearch-sample spec: Â version: 7.6.2 Â nodeSets: Â - name: default Â Â Â count: 1 Â Â Â config: Â Â Â Â Â node.master: true Â Â Â Â Â node.data: true Â Â Â Â Â node.ingest: true Â Â Â Â Â node.store.allow_mmap: false --- apiVersion: route.openshift.io/v1 kind: Route metadata: Â name: elasticsearch-sample spec: Â #host: elasticsearch.example.com # override if you don't want to use the host that is automatically generated by OpenShift (&amp;#60;route-name&amp;#62;[-&amp;#60;namespace&amp;#62;].&amp;#60;suffix&amp;#62;) Â tls: Â Â Â termination: passthrough # Elasticsearch is the TLS endpoint Â Â Â insecureEdgeTerminationPolicy: Redirect Â to: Â Â Â kind: Service Â Â Â name: elasticsearch-sample-es-http EOF&lt;/pre&gt; &lt;p&gt;Next, let us deploy a Kibana instance:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60;EOF | oc apply -n elastic -f - apiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata: Â name: kibana-sample spec: Â version: 7.6.2 Â count: 1 Â elasticsearchRef: Â Â Â name: "elasticsearch-sample" Â podTemplate: Â Â Â spec: Â Â Â Â Â containers: Â Â Â Â Â - name: kibana Â Â Â Â Â Â Â resources: Â Â Â Â Â Â Â Â Â limits: Â Â Â Â Â Â Â Â Â Â Â memory: 1Gi Â Â Â Â Â Â Â Â Â Â Â cpu: 1 --- apiVersion: v1 kind: Route metadata: Â name: kibana-sample spec: Â #host: kibana.example.com # override if you don't want to use the host that is automatically generated by OpenShift (&amp;#60;route-name&amp;#62;[-&amp;#60;namespace&amp;#62;].&amp;#60;suffix&amp;#62;) Â tls: Â Â Â termination: passthrough # Kibana is the TLS endpoint Â Â Â insecureEdgeTerminationPolicy: Redirect Â to: Â Â Â kind: Service Â Â Â name: kibana-sample-kb-http EOF&lt;/pre&gt; &lt;p&gt;We can now access the Kibana dashboard from the route exposed:&lt;/p&gt; &lt;pre&gt;$ oc get route -n elastic &lt;/pre&gt; &lt;p&gt;The credentials for logging into Kibana can be found under &lt;strong&gt;Secrets&lt;/strong&gt; for the Elastic project, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_706927" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret.png"&gt;&lt;img aria-describedby="caption-attachment-706927" class="wp-image-706927 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-1024x486.png" alt="The login credentials for Kibana are available in Secrets" width="640" height="304" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-1024x486.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-300x143.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-768x365.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706927" class="wp-caption-text"&gt;Figure 1: The login credentials for Kibana are available in Secrets.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&lt;b&gt;Setting up the Elasticsearch event emitterÂ &lt;/b&gt;&lt;/p&gt; &lt;p&gt;The event emitter integration code is a single Java class that implements the &lt;code&gt;EventEmitter&lt;/code&gt; interface. A basic implementation of the emitter &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/tree/master/jbpm-event-emitters/jbpm-event-emitters-elasticsearch"&gt;can be found here&lt;/a&gt;. By default process, task, and case metrics are pushed on to the corresponding indexes on Elastic:&lt;/p&gt; &lt;pre&gt;if (view instanceof ProcessInstanceView) { index = "processes"; type = "process"; id = ((ProcessInstanceView) view).getCompositeId(); } else if (view instanceof TaskInstanceView) { index = "tasks"; type = "task"; id = ((TaskInstanceView) view).getCompositeId(); } else if (view instanceof CaseInstanceView) { index = "cases"; type = "case"; id = ((CaseInstanceView) view).getCompositeId(); } content.append("{ \"index\" : { \"_index\" : \"" + index + "\", \"_type\" : \"" + type + "\", \"_id\" : \"" + id + "\" } }\n"); content.append(json); &lt;/pre&gt; &lt;p&gt;We can now extend this basic implementation and allow for HTTPS authentication because the Elastic instance on OpenShift is exposed over HTTPS:&lt;/p&gt; &lt;pre&gt;protected CloseableHttpClient buildClient() throws Exception{ Â Â HttpClientBuilder builder = HttpClients.custom(); Â Â if (elasticSearchUser != null &amp;#38;&amp;#38; elasticSearchPassword != null) { Â Â Â Â Â Â SSLContextBuilder builder1 = new SSLContextBuilder(); Â Â Â Â Â Â builder1.loadTrustMaterial(null, new TrustStrategy() { Â Â Â Â Â Â Â Â Â Â @Override Â Â Â Â Â Â Â Â Â Â public boolean isTrusted(X509Certificate[] chain, String authType) throws CertificateException { Â Â Â Â Â Â Â Â Â Â Â Â Â Â return true; Â Â Â Â Â Â Â Â Â Â } Â Â Â Â Â Â }); Â Â Â Â Â Â CredentialsProvider provider = new BasicCredentialsProvider(); Â Â Â Â Â Â UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(elasticSearchUser, elasticSearchPassword); Â Â Â Â Â Â provider.setCredentials(AuthScope.ANY, credentials); Â Â Â Â Â Â SSLConnectionSocketFactory sslConnectionSocketFactory = new SSLConnectionSocketFactory(builder1.build(), new NoopHostnameVerifier()); Â Â Â Â Â Â builder.setDefaultCredentialsProvider(provider); Â Â Â Â Â Â builder.setSSLSocketFactory(sslConnectionSocketFactory).build(); Â Â } Â Â return builder.build(); }&lt;/pre&gt; &lt;p&gt;Please note that for the purpose of this demonstration, we bypassed the certificate check and used the basic authentication mechanism. For a production use case, it is necessary to authenticate with a valid certificate.&lt;/p&gt; &lt;p&gt;We can now build a custom Docker image with the event emitter JAR on the KIE server warâ€™s classpath:&lt;/p&gt; &lt;pre&gt;FROM docker-registry.default.svc:5000/openshift/rhpam-kieserver-rhel8:7.5.0 COPY contrib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar /opt/eap/standalone/deployments/ROOT.war/WEB-INF/lib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar USER root RUN chownjboss:root /opt/eap/standalone/deployments/ROOT.war/WEB-INF/lib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar &amp;#38;&amp;#38; \ chmod 664 /opt/eap/standalone/deployments/ROOT.war/WEB-INF/lib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar USER jboss&lt;/pre&gt; &lt;p&gt;We also need to pass the corresponding metadata for the Elastic cluster using the &lt;code&gt;JAVA_OPTS&lt;/code&gt; Kie Server property:&lt;/p&gt; &lt;pre&gt;$ oc set env dc/{{ pam_app_name }}-kieserver JAVA_OPTS_APPEND=&amp;#62;\"-Dorg.jbpm.event.emitters.elasticsearch.url=https://{{routeelastic.stdout}} -Dorg.jbpm.event.emitters.elasticsearch.user=elastic -Dorg.jbpm.event.emitters.elasticsearch.password={{elasticpwd.stdout}}\" -n {{ OCP_PROJECT }}&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Business activity monitoring&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Now that we are done with the setup, let us quickly look at to visualize the process, task, and case metrics using Kibana. Let us begin by logging into Business Central and pulling a project from the &lt;strong&gt;Try Samples&lt;/strong&gt; section, as shown in Figures 2 and 3.&lt;/p&gt; &lt;div id="attachment_706947" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples.png"&gt;&lt;img aria-describedby="caption-attachment-706947" class="wp-image-706947 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-1024x446.png" alt="Business Central -&amp;#62; Space -&amp;#62; MySpace -&amp;#62; Projects -&amp;#62; dropdown list box -&amp;#62; Try Sample" width="640" height="279" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-1024x446.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-300x131.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-768x335.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706947" class="wp-caption-text"&gt;Figure 2: Pulling up the Try Samples section.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_706957" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples.png"&gt;&lt;img aria-describedby="caption-attachment-706957" class="wp-image-706957 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-1024x476.png" alt="The Try Samples project as it appears in MySpace" width="640" height="298" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-1024x476.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-300x139.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-768x357.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706957" class="wp-caption-text"&gt;Figure 3: The Try Samples section contains a variety of options.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;For this example, we will build and deploy the &lt;strong&gt;Mortgage_Process&lt;/strong&gt; project. We will use this project for metrics visualization. Let us now kickstart a process using the process start form:&lt;/p&gt; &lt;div id="attachment_706967" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form.png"&gt;&lt;img aria-describedby="caption-attachment-706967" class="wp-image-706967 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-1024x516.png" alt="" width="640" height="323" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-1024x516.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-300x151.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-768x387.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706967" class="wp-caption-text"&gt;Figure 4: Enter the details into the process start form.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The process is created, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_706977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation.png"&gt;&lt;img aria-describedby="caption-attachment-706977" class="wp-image-706977 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-1024x476.png" alt="MortgageApprovalProcess shown in the Process Instance perspective" width="640" height="298" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-1024x476.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-300x139.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-768x357.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706977" class="wp-caption-text"&gt;Figure 5: The MortgageApprovalProcess in the Process Instance perspective.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, log into the Kibana dashboard. We will start with a basic visualization for both processes and tasks. Kibana templates provide an exportable JSON format for sharing graphical reports across instances of Kibana. A sample template &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/ansible-rhpam-elastic/tree/master/kibana-resources"&gt;can be found here&lt;/a&gt;. Import the template from the Kibana dashboard&amp;#8217;s &lt;strong&gt;Saved Objects&lt;/strong&gt; section, under the &lt;strong&gt;Management&lt;/strong&gt; option.&lt;/p&gt; &lt;p&gt;We now have a sample dashboard available for processes (Figure 6) and tasks (Figure 7).&lt;/p&gt; &lt;div id="attachment_707007" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard.png"&gt;&lt;img aria-describedby="caption-attachment-707007" class="wp-image-707007 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-1024x468.png" alt="Dashboard showing the sample project's processes." width="640" height="293" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-1024x468.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-300x137.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-768x351.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707007" class="wp-caption-text"&gt;Figure 6: The new sample Processes dashboard.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_707017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard.png"&gt;&lt;img aria-describedby="caption-attachment-707017" class="wp-image-707017 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-1024x488.png" alt="Dashboard showing the sample project's tasks" width="640" height="305" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-1024x488.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-300x143.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-768x366.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707017" class="wp-caption-text"&gt;Figure 7: The new sample Tasks dashboard.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;As these tasks are worked on and the processes head to completion, their status and updates are pushed to Elastic, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_707027" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates.png"&gt;&lt;img aria-describedby="caption-attachment-707027" class="wp-image-707027 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-1024x497.png" alt="dashboard showing tasks broken down in 4 ways" width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-1024x497.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-300x146.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-768x372.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707027" class="wp-caption-text"&gt;Figure 8: Track the progress of your tasks and processes through their dashboards.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now let us create custom metrics from the process data. We can assume that we need to filter out the cases where property price is over 2 million and property age is less than 25 years. Kibana provides a convenient way to create full-text searches and then lets us convert the result to a visualization.&lt;/p&gt; &lt;p&gt;Open the raw data section of Kibana and create a custom query to filter out the data, as shown in Figure 9.&lt;/p&gt; &lt;div id="attachment_707037" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy.png"&gt;&lt;img aria-describedby="caption-attachment-707037" class="wp-image-707037 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-1024x403.png" alt="Kibana's raw section with two fields chosen and used in a custom query" width="640" height="252" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-1024x403.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-300x118.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-768x302.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707037" class="wp-caption-text"&gt;Figure 9: Create your custom filter.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can easily convert this data into a visualization by saving this search and creating a bar chart out of it, as shown in Figures 10 and 11.&lt;/p&gt; &lt;div id="attachment_707047" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz.png"&gt;&lt;img aria-describedby="caption-attachment-707047" class="wp-image-707047 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-1024x482.png" alt="Kibana's New Vertical Bar / Choose a source dialog box." width="640" height="301" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-1024x482.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-300x141.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-768x361.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707047" class="wp-caption-text"&gt;Figure 10: Create a bar chart.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_707057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph.png"&gt;&lt;img aria-describedby="caption-attachment-707057" class="wp-image-707057 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-1024x511.png" alt="Kibana showing your new bar chart." width="640" height="319" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-1024x511.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-300x150.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-768x383.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707057" class="wp-caption-text"&gt;Figure 11: Your new bar chart.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Similar visualizations can be created for cases, too. A sample case visualization using KPI metrics for the case can be set up as shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_707067" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz.png"&gt;&lt;img aria-describedby="caption-attachment-707067" class="wp-image-707067 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-1024x485.png" alt="Dashboard showing cases broken down in 4 ways" width="640" height="303" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-1024x485.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-300x142.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-768x364.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707067" class="wp-caption-text"&gt;Figure 12: The sample case visualization.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;A complete example of the setup &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/ansible-rhpam-elastic"&gt;can be found here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Summary&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;By setting up an integration with Elasticsearch, we can visualize data from the business automation engine side-by-side with the metrics from other disparate systems. Doing this also provides for faster, more scalable, business-friendly visualizations fit for operations management.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;References&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="http://mswiderski.blogspot.com/2017/08/elasticsearch-empowers-jbpm.html"&gt;Elasticsearch empowers jBPM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-openshift.html"&gt;Deploying Elastic and Kibana on Openshift&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#038;title=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" data-a2a-url="https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/" data-a2a-title="Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/"&gt;Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/BeZggEwPQzQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat Process Automation Manager is a platform for developing containerized microservices and applications that automate business decisions and processes. Combining process- and task-level SLA metrics plus case-related breakdowns can be beneficial for identifying trends and reorganizing the workforce as necessary. So, aÂ critical piece of a business process system is having real-time insights into what [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/"&gt;Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">706837</post-id><dc:creator>snandaku</dc:creator><dc:date>2020-05-04T07:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/</feedburner:origLink></entry><entry><title>The Week in JBoss [2020-04-30] - The Virtual Red Hat Summit week</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NE_f8rJykfA/the-week-in-jboss-2020-04-30-" /><category term="camel-k" scheme="searchisko:content:tags" /><category term="eda" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="keycloak" scheme="searchisko:content:tags" /><category term="Knative" scheme="searchisko:content:tags" /><category term="kogito" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><author><name>paul.robinson</name></author><id>searchisko:content:id:jbossorg_blog-the_week_in_jboss_2020_04_30_the_virtual_red_hat_summit_week</id><updated>2020-05-01T14:59:18Z</updated><published>2020-05-01T14:59:18Z</published><content type="html">&lt;!-- [DocumentBodyStart:037e93b8-4a46-4673-97d3-4435f15cb221] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;&lt;a href="https://www.redhat.com/en/summit" rel="nofollow"&gt;&lt;img alt="" class="image-1 jive-image" height="199" src="https://developer.jboss.org/servlet/JiveServlet/downloadImage/38-6364-439941/Screenshot+2020-05-01+at+15.03.04.png" style="width: 620px; height: 95px;" width="1301"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;I'm writing this as we wrap up another successful Red Hat Summit. But this year, with a significant distinction: the event was 100% virtual. Despite the involuntary move to virtual, there were many benefits that came about from the change in format. The event was completely free, and of course required no travel, allowing a much broader and more diverse set of attendees to benefit from the content and experience. It was also ran in three regions to accommodate many more timezones.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Being Red Hat we were keen to experiment with the format and find new ways to engage with the community. It was important for us to carry over as much of the personality and intimacy of the physical event as possible. So, this needed to be more than just a bunch of streamed talks. The talks were pre-recorded which allowed the presenter(s) to participate directly in the Q&amp;amp;A in real-time as the talk proceeded. There was also a variety of sessions that went beyond the talk format. For examples see &lt;em&gt;Ask the experts&lt;/em&gt;, &lt;em&gt;Networking social hour&lt;/em&gt;, and the &lt;em&gt;Virtual Open Neighborhood&lt;/em&gt; on the &lt;a class="jive-link-external-small" href="https://www.redhat.com/en/summit/agenda/agenda-at-a-glance" rel="nofollow"&gt;agenda&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you weren't able to attend, or want to catch some of the talks you missed, you can &lt;a class="jive-link-external-small" href="https://onlinexperiences.com/Launch/QReg.htm?ShowUUID=4245E6E3-7D25-496D-9B08-4CBDC87CCE74" rel="nofollow"&gt;re-live the virtual event here&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;In other news...&lt;/strong&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Quarkus Insights on Youtube Live&lt;/h2&gt;&lt;p&gt;This week Max Andersen and Emmanuel Bernard &lt;a class="jive-link-external-small" href="https://quarkus.io/blog/insights/" rel="nofollow"&gt;kicked off a new video/podcast series&lt;/a&gt; bringing insights into Quarkus. Each episode will focus on a guest speaker discussing the development or usage of Quarkus. There are also some dedicated Q&amp;amp;A sessions planned. Be sure to subscribe to the &lt;a class="jive-link-external-small" href="https://www.youtube.com/c/quarkusio" rel="nofollow"&gt;Quarkus YouTube channel&lt;/a&gt; to catch these sessions and other exciting Quarkus content. In particular &lt;a class="jive-link-external-small" href="https://www.youtube.com/watch?v=OCPFdpvL1Q0&amp;amp;feature=youtu.be" rel="nofollow"&gt;join them on the 4th of May&lt;/a&gt; where Georgios Andrianakis will talk about Quarkus testing and specifically the new mocking improvements in the recently released Quarkus 1.4.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Kogito: A Modular Codegen Design Proposal&lt;/h2&gt;&lt;p&gt;In &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/kogito_a_modular_codegen_design_proposal" rel="nofollow"&gt;this post&lt;/a&gt; Edoardo Vacchi explains how Kogito is improving performance by moving processing out of the run-time and into build-time.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Red Hat Summit 2020 - Ask the Experts: Hybrid Multicloud Pitfalls&lt;/h2&gt;&lt;p&gt;In one of the many &lt;em&gt;Ask the Experts&lt;/em&gt; sessions, Eric Schabell &amp;amp; Roel Hodzelmans focused on their hybrid multi-cloud pitfall theories. You can &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/red_hat_summit_2020_ask_the_experts_hybrid_multicloud_pitfalls_slides" rel="nofollow"&gt;view the slides here&lt;/a&gt;, or &lt;a class="jive-link-external-small" href="https://onlinexperiences.com/Launch/QReg.htm?ShowUUID=4245E6E3-7D25-496D-9B08-4CBDC87CCE74" rel="nofollow"&gt;register for the Red Hat Summit Virtual event&lt;/a&gt; to re-watch the content on demand.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Free book on Knative covering Camel K and Kafka and upcoming webinar with live demos&lt;/h2&gt;&lt;p&gt;In this post Claus Ibsen &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/free_book_on_knative_covering_camel_k_and_kafka_and_upcoming_webinar_with_live_demos" rel="nofollow"&gt;alerts us to the free eBook&lt;/a&gt; written by Burr Sutter &amp;amp; Kamesh Sampath on the subject of Knative. Go get your free copy &lt;a class="jive-link-external-small" href="https://developers.redhat.com/books/knative-cookbook/" rel="nofollow"&gt;here&lt;/a&gt;!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Red Hat Summit 2020 - Business Automation Sessions&lt;/h2&gt;&lt;p&gt;If you are interested in the area of &lt;em&gt;Business Automation&lt;/em&gt;, be sure to view Kris Verlaenen's &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/virtual_red_hat_summit_2020_april_28_29" rel="nofollow"&gt;helpful summary&lt;/a&gt; of all the BI related talks held at Red Hat Summit.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Demystifying the Event Driven Architecture - An introduction (part 1)&lt;/h2&gt;&lt;p&gt;Eric Schabell has &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/demystifying_the_event_driven_architecture_an_introduction_part_1" rel="nofollow"&gt;started a new blog series&lt;/a&gt; that explores the world of Event Driven Architectures (EDA).&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Six reasons why you will love Camel K&lt;/h2&gt;&lt;p&gt;Interested in Camel K, or want to find out what all the fuss is about? &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/six_reasons_why_you_will_love_camel_k" rel="nofollow"&gt;Read on&lt;/a&gt;, and Christina will give you six reasons to love Camel K.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Hybrid clouds with JGroups and Skupper&lt;/h2&gt;&lt;p&gt;Bela Ban follows up on his post explaining how to &lt;a class="jive-link-external-small" href="http://belaban.blogspot.com/2019/12/spanning-jgroups-kubernetes-based.html" rel="nofollow"&gt;span JGroups Kubernetes-based clusters across Google and Amazon clouds&lt;/a&gt;. In &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hybrid_clouds_with_jgroups_and_skupper" rel="nofollow"&gt;this new post&lt;/a&gt; Bela improves on the process by using &lt;a class="jive-link-external-small" href="https://skupper.io/" rel="nofollow"&gt;Skupper&lt;/a&gt; to simplify this task and encrypt the data exchanged between different clouds.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;This Week's Releases&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://quarkus.io/blog/quarkus-1-4-final-released/" rel="nofollow"&gt;Quarkus 1.4&lt;/a&gt;. Command mode, HTTP 2, New FaaS framework, Mocking, and more.&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://www.keycloak.org//2020/04/keycloak-1000-released.html" rel="nofollow"&gt;Keycloak 10.0.0&lt;/a&gt;. With &lt;em&gt;Identity Brokering Sync Mode&lt;/em&gt;, &lt;em&gt;Client Session Timeout for OpenID Connect / OAuth 2.0&lt;/em&gt; and much more.&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/kogito_0_9_1_released" rel="nofollow"&gt;Kogito 0.9.1&lt;/a&gt;. This release is a bug fix release, but there has also been considerable work spent on documentation and code examples. See the link for detais.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:037e93b8-4a46-4673-97d3-4435f15cb221] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NE_f8rJykfA" height="1" width="1" alt=""/&gt;</content><summary>Â  I'm writing this as we wrap up another successful Red Hat Summit. But this year, with a significant distinction: the event was 100% virtual. Despite the involuntary move to virtual, there were many benefits that came about from the change in format. The event was completely free, and of course required no travel, allowing a much broader and more diverse set of attendees to benefit from the conte...</summary><dc:creator>paul.robinson</dc:creator><dc:date>2020-05-01T14:59:18Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2020/05/01/the-week-in-jboss-2020-04-30-</feedburner:origLink></entry></feed>
